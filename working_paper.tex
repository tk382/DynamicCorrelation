\documentclass[aoas,preprint]{imsart}

\RequirePackage[OT1]{fontenc}
\RequirePackage{amsthm,amsmath}
\RequirePackage[numbers]{natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage[makeroom]{cancel}
\usepackage{bbm}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{float}
\newtheorem{theorem}{Theorem}
\usepackage{array}
\usepackage{enumerate}
\usepackage{color}
\usepackage{graphicx}
\usepackage{newfloat}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tocloft}
% settings
%\pubyear{2005}
%\volume{0}
%\issue{0}
%\firstpage{1}
%\lastpage{8}
\arxiv{arXiv:0000.0000}

\startlocaldefs
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\endlocaldefs

\begin{document}

\begin{frontmatter}
\title{Dynamic Gene coexpression Analysis with Correlation Modeling}
\runtitle{Dynamic Gene coexpression Analysis}

\begin{aug}
\author{\fnms{Tae} \snm{Kim}\ead[label=e1]{tk382@uchicago.edu}}
\and
\author{\fnms{Dan} \snm{Nicolae}\ead[label=e2]{nicolae@uchicago.edu}}

\affiliation{University of Chicago}
\address{5747 South Ellis Avenue\\
Chicago, IL 60637}
\end{aug}

\begin{abstract}
% Studies on differential network analysis have focused on discrete populations rather than a continuously varying one. To fill this gap, we propose a way to model the covariance against a continuous covariate to address a wide range of differential network problems. We expand the past works of variance modeling (heteroskedasticity) to covariance modeling and propose a test statistic with nice asymptotic properties. Subsequently, we introduce a way to combine these test statistics to look at the hub nodes in particular to obtain a more global view of the network behavior. The simulations show that the proposed method, compared to alternative methods, performs well in terms of statistical power under more diverse scenarios of heteroskedasticity with much less computational cost. We apply this to GTEx data to analyze the dynamic behavior of the African Americans' gene-coexpression network against their genetic ancestry, and we identify transcription factors whose connectivity to their target genes changes with respect to the genetic ancestry.


% Most standard statistical models assume that the residual variance is constant, while many applications such as network analysis requires a tool to model the variance term against a set of predictors. Here, we expand on past works that model the variance to modeling covariance between two variables. We devise a test statistic to see whether any of the predictors has a non-zero effect on the covariance. Subsequently, we introduce a way to combine these test statistics to look at analyze the relationship between one variable and multiple others. The simulations show that the proposed method, compared to alternative methods, performs well in terms of statistical power under more diverse scenarios of underlying model with much less computational cost. We apply this to GTEx data to analyze the dynamic behavior of the African Americans' gene-coexpression network against their genetic ancestry, and we identify transcription factors whose covariance with their target genes changes with respect to the genetic ancestry.

% Most standard statistical models assume that the residual variance is constant, while many applications such as network analysis requires a tool to model the variance term against a set of predictors. Here, we expand on past works that model the variance to modeling covariance between two variables. We devise a test statistic to see whether any of the predictors has a non-zero effect on the covariance. Subsequently, we introduce a way to combine these test statistics to look at analyze the relationship between one variable and multiple others. The simulations show that the proposed method, compared to alternative methods, performs well in terms of statistical power under more diverse scenarios of underlying model with much less computational cost. We apply this to GTEx data to analyze the dynamic behavior of the African Americans' gene-coexpression network against their genetic ancestry, and we identify transcription factors whose covariance with their target genes changes with respect to the genetic ancestry.

% We aim to study the effects of genetic differences in gene coexpression networks among different populations. One possible way is to compare discrete populations with existing differential network analysis tools, but such analysis would suffer from confounding effects of hidden environmental variables. Therefore, we develop a method that models the covariance between two genes to continuously vary with the genetic ancestry of African Americans. We tackle the following three main statistical challenges. First, since we are modeling covariance, we can no longer rely on the linearity assumption of predictor effect. Second, it is difficult to impose a specific structure on a continuously-varying covariance matrix. Third, the method must computationally scale well so that it can be applied to large genomic data sets. Subsequently, we propose a way to combine these test statistics to analyze the relationship between one gene and multiple others to obtain a more global view of the network behavior. Simulations show that the proposed method, compared to alternative methods, performs well in terms of statistical power under more diverse scenarios with much less computational cost. We apply this to GTEx data to analyze the dynamic behavior of the African Americansâ€™ gene coexpression network against their genetic ancestry, and we identify transcription factors whose coexpression with their target genes changes with the genetic ancestry. We believe this method can be applied to a wide array of network problems.

To better understand the difference in biological processes among populations, we aim to study the effect of genetic ancestry on gene coexpression. We propose a method that can model the covariance between two variables to vary against a continuous covariate. We apply variable transformation to utilize past works on heteroskedasticity, and we offer a score test statistic that is robust to model misspecification, computationally simple, and inferentially unaffected by low sample size. Subsequently, we expand the method to test relationships between one hub-gene and many other genes to obtain a more global view of the coexpression network. Simulations show that the proposed method, compared to alternatives, has higher statistical power under more diverse scenarios with much less computational cost. We apply this method to GTEx data to analyze the dynamic behavior of the African Americans' gene coexpression against their genetic ancestry, and we identify transcription factors whose coexpressions with their target genes change with the genetic ancestry. We believe this method can be applied to a wide array of problems that require covariance modeling.
\end{abstract}

\begin{keyword}
coexpression; network; heteroskedasticity; score test; GTEx; admixed population;
\end{keyword}

\end{frontmatter}
% \begin{enumerate}
%     \item Introduction
%     \begin{enumerate}
%         \item Data sets have become more and more complicated, and there's need for covariance modeling for multivariate data. It's an important statistical problem.
%         \item One example is coexpression network. It's an important biological problem.
%         \item Literature search 1: Differential Correlation and Liquid association 
%         \item Literature search 2: Works in heteroskedasticity. It addresses different problem, but similar in flavor that we can potentially take advantage of.
%         \item Proposed method
%         \item Paper organization
%     \end{enumerate}
%     \item Methods
%     \begin{enumerate}
%         \item Framework
%         \begin{enumerate}
%             \item Model and Notation
%             \item Statistical problem (how to model $\Sigma$) and interpretation
%             \item Reasoning for pair-wise modeling
%         \end{enumerate}
%         \item Inference for $K=2$
%         \begin{enumerate}
%             \item Reasoning for variable transformation
%             \item Two possibilities: Likelihood Ratio test vs Score test
%             \item derivation of score statistic
%             \item Small sample correction
%         \end{enumerate}
%         \item Connection to Liquid Association
%         \begin{enumerate}
%             \item Definition of LA
%             \item Similarity between LA and score test
%             \item Difference between them and why ours is better
%         \end{enumerate}
%         \item Inference for $K>2$
%         \begin{enumerate}
%             \item Set-up (global null)
%             \item Justification for combining test statistics
%             \item Derivation of the new test statistic
%             \item When $K$ is greater than $n$: permutation
%         \end{enumerate}
%         \item Generalization to Non-unit Variance
%         \begin{enumerate}
%             \item When $K=2$
%             \item When $K>2$
%         \end{enumerate}
%     \end{enumerate}
%     \item Simulation Studies
%     \begin{enumerate}
%         \item performance comparison score vs LR vs LA under Fisher model and under quadratic model
%         \item \textit{one example under null and alternative hypothesis for $K>2$}
%     \end{enumerate}
%     \item Application to GTEx Data
%     \begin{enumerate}
%         \item Data
%         \item Analysis (Transcription factors against their targets)
%         \item Results (One with the lowest $p$-values)
%     \end{enumerate}
%     \item Discussion
%     \begin{enumerate}
%         \item \textit{Summary}
%         \item \textit{Limitations: (1) Bivariate normal, (2) For cases $K>n>2$, the burden of permutation test and difficult inference (3) Modeling matrix instead of covariance}
%         \item \textit{Applications outside coexpression network (social network, etc.)}
%     \end{enumerate}
% \end{enumerate}

\section{Introduction}
\subsection*{}
Gene coexpression is widely studied to understand how genes are functionally connected and to often provide insights into analyzing the transcriptional regulatory system. This biological system is extremely complicated to fully and accurately comprehend when available data is limited, but we can still get vital pieces of information by focusing on a few key genes and study how they are connected to their other genes. For example, we can isolate one transcription factor gene and study how it is connected to its target genes. We define this quality of a transcription factor as its "local connectivity." This biological problem leads us to the next problem: how does local connectivity vary across various phenotypic conditions? Past works have studied how subjects in different disease statuses show distinct coexpression patterns, contributing to a better understanding of the disease at a molecular level \cite{de2010differential}. Another phenotype of interest is the genetic ancestry of admixed population. Genetic ancestry is already known to play a critical role in other molecular phenotypes including DNA methylation and gene expressions \cite{price2008effects, galanter2017differential}, and so we believe it has an important role in the inter-gene relationships as well. Therefore, in this paper, we study how the local connectivity of transcription factor genes changes with ancestry. Specifically, we study the gene coexpression of African American subjects to identify candidate transcription factors whose effects on their targets vary with the proportion of African ancestry in their genome. This analysis will help us better comprehend how genes are differentially regulated in distinct populations. \bigskip

The above problem can be translated into a statistical problem of modeling the covariance matrix of the expression levels of multiple genes. We can start from its simplest form by studying the expression levels of two genes. We build a statistical model that can explain how their correlation varies against genetic ancestry. This problem can easily be generalized into any covariance modeling problem for bivariate data outside the field of genetics. Variance modeling has been widely studied in the context of heteroskedasticity \cite{breusch1979simple, glejser1969new, white1980heteroskedasticity}, and correlation modeling under discrete conditions has been studied in the context of differential network \cite{ideker2012differential}, but dynamic correlation modeling has been less explored. \bigskip

Li (2002) addresses the most similar scientific problem to ours \cite{li2002genome, li2004system}. The paper uses the term ``liquid association" (LA) to conceptualize the internal evolution of the coexpression pattern for a pair of genes. He analyzes the coexpression that changes across the different cellular states that cannot be directly observed by using the expression level of another gene to represent the cellular state. Other studies built on the liquid association to better identify cell states that affect coexpression \cite{dunlop2008regulatory, yu2018new, yan2017detecting}, most focusing on expanding the test to genome-scale. However, methods based on liquid association have some limitations. First, it restricts the covariate to be a 1-dimensional vector, and cannot be generalized to test local ancestry. Second, it treats the covariate as a random variable that follows a normal distribution, which genetic ancestry does not. Third, it only tests the linear relationship between the covariate and the coexpression. Lastly, the proposed test statistic does not have a closed-form null distribution and requires a permutation test, leading to computational inefficiency. \bigskip

We propose a different methodology to solve the continuously-varying covariance problem. We first assume that the two genes' expression levels follow a bivariate normal distribution. Then, we apply a simple variable transformation to induce independence, effectively changing the multivariate covariance modeling problem to a univariate variance modeling problem. Next, we apply a traditional score test for heteroskedasticity \cite{breusch1979simple} where the null hypothesis is that the coexpression does not vary with the covariate. This method is generalizable to non-normal, multivariate covariates, and it is also applicable to a non-linear relationship between the variance and the covariate. Moreover, the score test statistic asymptotically follows a chi-squared distribution, and hence it is easily expandable to a large number of tests without excessive computational burden. Subsequently, we tackle the local connectivity problem by expanding the scope of the problem from the relationship of two genes to the relationships between one gene and multiple genes. When the number of genes is smaller than the sample size, the desired statistical properties apply to the new combined test statistic as well. \bigskip

The rest of the paper is organized as follows. First, we lay out the framework for the score test that tests whether the covariance between bivariate normal variables varies against a continuous covariate $X$. Then we propose a way to combine the pair-wise test statistics for one gene and test the global null that the local connectivity of one variable does not change with genetic ancestry. Next in the simulation section, we show that the proposed method has distinct advantages compared to alternatives such as the likelihood ratio test or liquid association. Finally, we share our real data analysis results using GTEx data for African Americans' transcriptome and genome. We end with a discussion about limitations of the method, possible future directions, and potential applications to fields outside genetics.\bigskip

\section{Methods}
\subsection{Framework for 2 genes \label{sec:framework2}}
We assume the data $\bm{y}_i = \mathbb{R}^2$, $i = 1, 2, \cdots, N$ independently follows bivariate normal distribution. $\bm{y}_i$ is the gene expression level of two genes of individual $i$. The covariate matrix $X \in \mathbb{R}^{N \times P} = \{x_{ip}\}_{i=1,p=1}^{N,P}$ is assumed full-rank with $P < N$. This is a P-dimensional representation of genetic ancestry for each individual $i = 1, \cdots, N$. 
\begin{equation}
\begin{multlined}
    \begin{bmatrix} y_{i1} \\ y_{i2} \end{bmatrix} = 
    \begin{bmatrix} b_{1} \\ b_{2} \end{bmatrix} + 
    \begin{bmatrix} \bm{x}_i^T \bm{\beta}_1 \\ \bm{x}_i^T \bm{\beta}_2 \end{bmatrix} + 
    \begin{bmatrix} {u_{i1}} \\ {u_{i2}} \end{bmatrix}\\
    \begin{bmatrix} {u_{i2}} \\ u_{i2} \end{bmatrix} 
    \sim \mathcal{N}\left(
        \begin{bmatrix} 0 \\ 0 \end{bmatrix}, 
        \begin{bmatrix} \rho_{11} & \rho_{12}(\bm{x}_i, \bm{\alpha}) \\ 
        \rho_{12}(\bm{x}_i, \bm{\alpha}) & \rho_{22} \end{bmatrix}
    \right)
    \end{multlined}
    \label{eq:framework_2genes}
\end{equation}
$\bm{\beta}_1$ and $\bm{\beta}_2$ are coefficients with length $P$  for the mean term. $\rho_{11}$ and $\rho_{22}$ are fixed scalars but $\rho_{12}$ is a function of $\bm{x}$ and unknown parameter of interest $\bm{\alpha}$ with length $P$. \\

Above model is close to a multivariate regression model of $\bm{y}_i$ against covariate $\bm{x}_i$ with intercept $\bm{b}_0$, slope $\begin{bmatrix} \bm{\beta}_1 & \bm{\beta}_2 \end{bmatrix}$, and error term $\bm{u}_i$ but is different in that its error variance depends on the covariate $\bm{x}_i$ through some parameter $\bm{\alpha}$. The function $\rho_{12}$ represents a specific but unknown form of heteroskedasticity between the two variables $1$ and $2$. We additionally assume that the diagonal entries of $\Sigma$ are fixed and do not depend on $\bm{x}_i$. This may or may not be appropriate depending on the contexts, but it is empirically justified for our example data set of GTEx gene expression level matrix. In the context of gene coexpression of African Americans, $\bm{y}_i$ is gene expression level of an African American individual $i$ at two selected genes. The covariate $\bm{x}_i$ holds genetic ancestry information of individual $i$. It can be a scalar that represents the proportion of African ancestry in the genome, a vector of the first few principal components of the genotypes, or a vector of local ancestry at multiple loci. In the application section (\ref{applications}), we focus on scalar $\bm{x}_i$ for straightforward interpretability. \\

The parameter of interest is $\bm{\alpha}$, and others $(\bm{b}$, $\bm{\beta}$, $\rho_{11}$, $\rho_{22}$) are nuisance parameters. Here, we develop a method to test the following null hypothesis.
\begin{equation}
    \bm{\alpha} = \bm{0}
    \label{eq:null}
\end{equation}

Although we can work directly with bivariate normal likelihood from \ref{eq:framework_2genes}, we propose a variable transformation to make the likelihood much simpler without losing any information. 

\begin{align} \begin{split}
\hspace{-5mm}& \begin{bmatrix} w_{i} \\ v_{i} \end{bmatrix} = 
\begin{bmatrix} \frac{y_{i1}}{\sqrt{\rho_{11}}} + \frac{y_{i2}}{\sqrt{\rho_{22}}} \\ \frac{y_{i1}}{\sqrt{\rho_{11}}} - \frac{y_{i2}}{\sqrt{\rho_{22}}} \end{bmatrix} = \\
& \hspace{20mm} 
 \begin{bmatrix}
  \frac{{b}_1}{\sqrt{\rho_{11}}} + \frac{{b}_2}{\sqrt{\rho_{22}}}\\
  \frac{{b}_1}{\sqrt{\rho_{11}}} - \frac{{b}_2}{\sqrt{\rho_{22}}}
 \end{bmatrix} + 
 \bm{x}_i^T \begin{bmatrix} \frac{\bm{\beta}_1}{\sqrt{\rho_{11}}} + \frac{\bm{\beta}_2}{\sqrt{\rho_{22}}} \\ \frac{\bm{\beta}_1}{\sqrt{\rho_{11}}} - \frac{\bm{\beta}_2}{\sqrt{\rho_{22}}} \end{bmatrix} + \begin{bmatrix} u_{wi} \\ u_{vi}\end{bmatrix}\\
&  \begin{bmatrix} u_{wi} \\ u_{vi}\end{bmatrix} \sim  \mathcal{N} \left( \bm{0}, \begin{bmatrix} 2 + \frac{2\rho_{12}(\bm{x}_i^T\bm{\alpha})}{\sqrt{\rho_{11}\rho_{22}}} & 0 \\ 0 & 2 - \frac{2\rho_{12}(\bm{x}_i^T\bm{\alpha})}{\sqrt{\rho_{11}\rho_{22}}} \end{bmatrix} \right)
 \end{split} 
  \label{eq:transformed_2genes}
  \end{align}
 For notational convenience, we define the following two variance parameters.
 \begin{equation}
     \sigma_{wi}^2 = 2 + \frac{2\rho_{12}(\bm{x}_i^T \bm{\alpha})}{\sqrt{\rho_{11}\rho_{22}}}, \hspace{6mm}
     \sigma_{vi}^2 = 2 - \frac{2\rho_{12}(\bm{x}_i^T \bm{\alpha})}{\sqrt{\rho_{11}\rho_{22}}}
 \end{equation}
Now, the two variables $w_i$ and $v_i$ are independent, and the bivariate likelihood is equal to the sum of two univariate likelihoods.

\begin{equation}
\begin{multlined}
\ell(\bm{\alpha}, \bm{b}, \bm{\beta}, \rho_{11}, \rho_{22}) = \\
-\frac{N}{2} log(2\pi) - \frac{1}{2} \sum_{i=1}^{N} log (\sigma_{wi}^2) - \sum_{i=1}^{N} \frac{(w_i - \mu_{wi})^2}{\sigma_{wi}^2}\\
-\frac{N}{2} log(2\pi) - \frac{1}{2} \sum_{i=1}^{N} log (\sigma_{vi}^2) - \sum_{i=1}^{N} \frac{(v_i - \mu_{vi})^2}{\sigma_{vi}^2}
\end{multlined}
    \label{eq:likelihood_2genes}
\end{equation}

where 
$$\mu_{wi} = \frac{{b}_1}{\sqrt{\rho_{11}}} + \frac{{b}_2}{\sqrt{\rho_{22}}} + \bm{x}_i^T \left(\frac{\bm{\beta}_1}{\sqrt{\rho_{11}}} + \frac{\bm{\beta}_2}{\sqrt{\rho_{22}}}\right)$$
$$\mu_{vi} = \frac{{b}_1}{\sqrt{\rho_{11}}} - \frac{{b}_2}{\sqrt{\rho_{22}}} + \bm{x}_i^T \left(\frac{\bm{\beta}_1}{\sqrt{\rho_{11}}} - \frac{\bm{\beta}_2}{\sqrt{\rho_{22}}}\right)$$

This transformation effectively changes the covariance modeling problem to variance modeling problem, and we can apply the results from literature on univariate heteroskedasticity. Given the likelihood \ref{eq:likelihood_2genes}, we have two well-known tools to test the null hypothesis \ref{eq:null}: likelihood ratio test and Rao's score test \cite{breusch1979simple}. \\

% Both methods require us to impose certain assumptions on $\rho$, but Rao's score test allows much more flexibility.\\

One one hand, likelihood ratio test requires the full specification of the function $\rho$ to estimate the maximum likelihood estimate (MLE) of $\bm{\alpha}$ both under the null hypothesis and under the alternative hypothesis. One straightforward function for $\rho$ is any kind of sigmoid function bounded to (-1, 1) such as logistic function, hyperbolic tangent function, or any cumulative distribution supported on the whole real line. For the input of $\rho$, we can use simple linear form of $\bm{x}_i^T\bm{\alpha}$, or we can also allow non-linearity by using higher order polynomial or even generalized additive models. There are two problems with the likelihood ratio test. One, as mentioned in the previous section, we would like to impose as little assumption on the specific form of heteroskedasticity as possible. If $\rho$ is highly mis-specified, we sacrifice a lot of statistical power. Two, most of the reasonable assumptions of $\rho$, such as the sigmoid functions mentioned above, do not lead to a closed form MLE of $\bm{\alpha}$ under the alternative hypothesis. It would require us to numerically optimize the likelihood, leading to computational inefficiency, especially when the test space is large as in our application of gene coexpression. \\

On the other hand, Rao's score test, unlike the likelihood ratio test, only requires the MLE of $\bm{\alpha}$ under the null hypothesis \cite{rao1973linear}. Moreover, under a mild assumption of linearity and additivity ($\rho(\bm{x}_i, \bm{\alpha}) = \rho(\bm{x}_i^T\bm{\alpha})$), the test statistic does not depend on the form of $\rho$ while maintaining its asymptotic property as long as $\rho$ is twice differentiable. Therefore, we define $\bm{\alpha} \in \mathbb{R}^{p}$ as the linear coefficients and finalize our model below
\begin{equation}
    \rho_{12}(\bm{x}_i, \bm{\alpha}) = \rho_{12}(\bm{x}_i^T\bm{\alpha}). \label{rho}
\end{equation}
However, we believe the linearity and additivity assumptions are standard for the incipient stage of the analysis. Also, since $\rho$ can take any non-linear form, \ref{rho} still is a flexible framework  especially compared to the likelihood ratio test. In order to test \ref{eq:null}, we expand the result from Breusch and Pagan (1979) to derive the test statistic \cite{breusch1979simple}.\\ %The performance of score test and the likelihood ratio test are further analyzed in the simulation section \ref{simulations}\\

The score test allows us to replace all the nuisance parameters with the MLEs under the null hypothesis. We can therefore replace $\bm{\beta}$, $\bm{b}$, $\rho_{11}$, and $\rho_{22}$ with their respective MLEs that result from ordinary least squares linear regression. The first derivative of the likelihood evaluated at $\bm{\alpha}=\bm{0}, \bm{\beta} = \bm{\hat{\beta}}$, $\bm{b} = \bm{\hat{b}}$, $\rho_{11} = \hat{\rho}_{11}$, and $\rho_{22} = \hat{\rho}_{22}$ is
\begin{equation}
\begin{multlined}
\bm{\tilde{d}_{\alpha}} = \frac{\partial \ell(\bm{\alpha}, \bm{b}_0, \bm{\beta})} {\partial \bm{\alpha}}\mid_{\bm{\alpha}=\bm{0}, \bm{\beta} = \bm{\hat{\beta}}, \bm{b} = \bm{\hat{b}}, \rho_{11} = \hat{\rho}_{11}, \rho_{22} = \hat{\rho}_{22}} =\\
-\frac{\rho_{12}'(\bm{0})}{2\sqrt{\rho_{11}\rho_{22}}} \sum_{i=1}^{N} \left(
    \frac{\bm{x}_i}{\hat{\sigma}_{w}^2} 
    \left( 1-\frac{\hat{u}_{wi}^2}{\hat{\sigma}_{w}^2}\right) - 
    \frac{\bm{x}_i}{\hat{\sigma}_{v}^2} 
    \left( 1-\frac{\hat{u}_{vi}^2}{\hat{\sigma}_{v}^2}\right) 
    \right)
\end{multlined}
\label{first_derivative}
\end{equation}
where $\hat{\sigma}_w^2 = \sum_{i=1}^{N} \hat{u}_w^2/N$ and $\hat{\sigma}_v^2 = \sum_{i=1}^{N}\hat{u}_v^2/N$ as the error variance from linear regression. Similarly, $\hat{u}_{wi}^2$ and $\hat{u}_{vi}^2$ are the residuals from the linear regression of \ref{eq:transformed_2genes}. The Fisher information for $\bm{\alpha}$ is
\begin{equation}
\begin{multlined}
        \tilde{\mathcal{I}}_{\bm{\alpha}\bm{\alpha}^T} = \mathcal{I}_{\bm{\alpha}\bm{\alpha}^T}\mid_{
        \bm{\alpha}=\bm{0}, 
        \bm{b} = \bm{\hat{b}},
        \bm{\beta} = \bm{\hat{\beta}},
        {\rho}_{11} = \hat{\rho}_{11},
        {\rho}_{22} = \hat{\rho}_{22}}\\
        = \frac{{\rho '}^2_{12}(\bm{0})}{4\hat{\rho}_{11}\hat{\rho}_{22}}\cdot 2 \cdot \left(\frac{1}{\hat{\sigma}_{w}^2} + \frac{1}{\hat{\sigma}_{v}^2} \right) \sum_{i=1}^{N} \bm{x}_i \bm{x}_i^T
    \end{multlined}
\end{equation}
All other second-order derivatives are $\bm{0}$ when the covariates are centered. The test statistic $q$ for the variables 1 and 2 is
\begin{equation}
    \begin{multlined}
    q = \tilde{d}_{\bm{\alpha}}^T \tilde{I}_{\bm{\alpha}\bm{\alpha}^T}^{-1}
    \tilde{d}_{\bm{\alpha}}
    =\frac{1}{2}\left( \frac{1}{\hat{\sigma}_w^4}+\frac{1}{\hat{\sigma}_v^4}\right)^{-1}
\left(\sum_{i=1}^{N} \bm{x}_{i} 
\left(
\frac{\hat{\sigma}_w^2-\hat{u}_{wi}^2}{\hat{\sigma}_w^4}- \frac{\hat{\sigma}_v^2-\hat{u}_{vi}^2}{\hat{\sigma}_v^4}
\right)
\right)^T \\
\left( \sum_{i=1}^{N} \bm{x}_i^T \bm{x}_i \right)^{-1}
\left(
\sum_{i=1}^{N} \bm{x}_{i} 
\left(
\frac{\hat{\sigma}_w^2-\hat{u}_{wi}^2}{\hat{\sigma}_w^4}- \frac{\hat{\sigma}_v^2-\hat{u}_{vi}^2}{\hat{\sigma}_v^4}
\right)
\right)
    \end{multlined}
    \label{eq:q}
\end{equation}
where the unknown function $\rho$ has been canceled out. Every component of the test statistic is easily acquired from the data, and the computational burden is low. Most importantly, it is flexible as it allows any form of heteroskedasticity $\rho$. Under this setting, Breusch and Pagan (1979)  proves that $q$ asymptotically follows $\chi_{P}^2$ \cite{breusch1979simple}.
\begin{equation}
q \rightarrow \chi_P^2
\label{asymptotic_q}
\end{equation}
\begin{theorem}
Consider the framework in \ref{eq:framework_2genes}. Let the following assumptions in ordinary least squares hold. 
\begin{itemize}
    \item The error term $\begin{bmatrix}u_{i1} & u_{i2} \end{bmatrix}^T$ for $i = 1, \cdots, N$ are independent, are bivariate normal, and have mean $\bm{0}$. 
    \item The diagonal terms of the error variance, $\rho_{11}$ and $\rho_{22}$, are constant.
    \item $X$ is full rank.
\end{itemize}
Additionally, let all the covariates be centered, so that $\sum_{i=1}^{N} x_{ip} = 0$ for $p = 1, \cdots, P$. Also, the non-dinagonal term $\rho_{12}$ is a function of linear combination of covariate $\bm{x}_i$ and unknown parameter $\bm{\alpha}$. Then, the score statistic $q$ tests the null hypothesis in \ref{eq:null} defined in \ref{eq:q} and $q$ asymptotically follows $\chi_P^2$. 
\label{theorem:q}
\end{theorem}
However, even though the introduced test statistic has convenient asymptotic properties, the sample size is not large enough in many applications. The error is in the order of $N^{-1}$ \cite{harris1985asymptotic}, and many Monte Carlo experiments show that the test rejects the null hypothesis less frequently than indicated by its nominal size \cite{ godfrey1978testing, griffiths1986monte,honda1988size} . In response, corrections have been suggested \cite{cribari2001monotonic,harris1985asymptotic,  honda1988size}, and we apply Honda's method to ensure the asymptotic properties even under the samll sample size. The details of the small-sample correction as well as the derivation of the test statistic, are shared in the Appendix.\\

%  In practice, we can scale each column of the data matrix $Y$ by its maximum likelihood estimate of variance as the first step of the analysis.
% $$\tilde{Y} = \begin{bmatrix} \tilde{\bm{y}}_{\cdot 1}, \cdots, \tilde{\bm{y}}_{\cdot K} \end{bmatrix}$$
% $$\tilde{\bm{y}}_{\cdot k} = \frac{\bm{y}_{\cdot k}}{\sqrt{\sum_{i=1}^{N} y_{ik}^2/N}}$$

%  Replacing the data matrix $Y$ with its scaled version $\tilde{Y}$, all the testing and inference procedures introduced in the previous sections remain the same.




% \subsection{Framework}
% We assume the data $\bm{y_i} \in \mathbb{R}^{K}$, $i=1, 2, \cdots, N$ are independent, and we define the data matrix $Y = \{y_{ik}\}_{i=1, k=1}^{N, K} \in \mathbb{R}^{N \times K}$. The covariate matrix $X \in \mathbb{R}^{N \times P} = \{x_{ip}\}_{i=1, p=1}^{N, P}$ is assumed to be full-rank with $P < N$. We assume $\bm{y_i}$ independently follow $K$-variate normal distribution as follows,

% \begin{equation}
% \bm{y_i} = \bm{b}_0 + \bm{x}_i^TB + \bm{u_i}, \bm{u_i} \sim \mathcal{N}_K(\bm{0}, \Sigma(\bm{x}_i^T\bm{\alpha}))
% \label{eq_framework}
% \end{equation}
% $$\Sigma (\bm{x}_i) = \{\rho_{kl}(\bm{x}_i^T\bm{\alpha})\}_{k, l = 1}^K$$

%  where $\rho_{kl}$ is constant when $k = l$ while varies across $\bm{x}_i$ only when $k \neq l$. \\

%  The above model is close to a multivariate regression model of $\bm{y_i}$ with respect to the covariate $\bm{x}_i$ with intercept $\bm{b}_0$, slope $B$, and error term $\bm{u_{i}}$. However, it differs from the classical regression model because its error variance $\Sigma(\bm{x}_i, \bm{\alpha})$ depends on the covariate $\bm{x}_i$ through some parameters $\bm{\alpha}_{k\ell}$. The function $\rho_{k\ell}$ represents the specific form of heteroskedasticity between the two variables $k$ and $\ell$, and possible choices for this function are further discussed in section \ref{pairwise}. \\

%  In the context of gene-coexpression network of African Americans, the data matrix $Y$ is the gene expression level matrix for $N$ individuals at $K$ genes, and therefore, we expect this to be a high dimensional problem with $K>N$. The covariate $\bm{x}_i$ holds genetic ancestry information of individual $i$. It can be a scalar that represents the proportion of African ancestry in the genome, a vector of the first few principal components of the genotypes, or a vector of local ancestry at multiple loci. In the application section \ref{applications}, we focus on scalar $\bm{x}_i$ for straightforward interpretability. \\

%  We aim to model $\Sigma$ to see how the relationship among $K$ variables changes across the different $\bm{x}_i$. and therefore we assume that the diagonal entries of $\Sigma$ are fixed and do not depend on $\bm{x}_i$. This may or may not be appropriate depending on the contexts, but it is empirically justified for our example data set of GTEx gene expression level matrix and genetic matrix.  \\



% \subsection{Inference for $K = 2$} \label{pairwise}
% Here, we focus on a simple case of $K=2$. For the next two sections, we assume that the variance matrix has 1 on all the diagonals for mathematical simplicity. This can be a proper choice for certain applications. For example, the gene expression level has already been quantile normalized as a pre-processing step. In section \ref{unscaled}, we lift the unit variance assumption and present the method for the general case.\\

% We can re-write (\ref{eq_framework}) for $K=2$ as below,

% \begin{equation}
% \begin{multlined}
%     \begin{bmatrix} y_{i1} \\ y_{i2} \end{bmatrix} = 
%     \begin{bmatrix} b_{01} \\ b_{02} \end{bmatrix} + 
%     \begin{bmatrix} \bm{x_i^T \beta_1} \\ \bm{x_i^T \beta_2} \end{bmatrix} + 
%     \begin{bmatrix} {u_{i1}} \\ {u_{i2}} \end{bmatrix}\\
%     \begin{bmatrix} {u_{i2}} \\ u_{i2} \end{bmatrix} \sim \mathcal{N}\left(
%         \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 & \rho_{12}(\bm{x}_i, \bm{\alpha_{12}}) \\ \rho_{12}(\bm{x}_i, \bm{\alpha_{12}}) & 1 \end{bmatrix}
%     \right)
%     \label{kis2}
%     \end{multlined}
% \end{equation}
%  where $\bm{\beta}_1$ and $\bm{\beta}_2$ are the first and second columns of $B$ respectively. The null hypothesis is 
% \begin{equation}
% \rho_{12}(\bm{x}_i, \bm{\alpha_{12}})= \bar{\rho}_{12}.\label{null1}
% \end{equation}
% In other words, the covariance between variables $1$ and $2$ do not depend on $\bm{x}_i$ and is a constant value $\bar{\rho}_{12}$. Both the function $\rho$ and the parameters $\bm{\alpha}$ depend on the variable pair $1$ and $2$, and for the rest of the section, we drop the subscripts $12$ for notational convenience. \\

%  The likelihood of (\ref{kis2}) is
% \begin{equation}
% \begin{multlined}
%     \ell(\bm{\alpha},\bm{b}_0,\bm{\beta}) = \sum_{i=1}^{N} \bigg(
%     -log(2\pi) - \frac{\sqrt{1-\rho^2(\bm{x}_i, \bm{\alpha})}}{2}\\
%     \shoveleft[3cm] - \frac{(y_{i1}-\mu_{y_1})^2 + (y_{i2}-\mu_{y_2})^2 - 2\rho(\bm{x}_i, \bm{\alpha})(y_{i1}-\mu_{y_1})(y_{i2}-\mu_{y_2})}{2(1-\rho^2(\bm{x}_i, \bm{\alpha}))} \bigg),
% \end{multlined}
% \label{bivariate_lik}
% \end{equation}
% where $\mu_{y_1} = b_{01} + \bm{x}_i^T \bm{\beta}_1$,  $\mu_{y_2} = b_{02} + \bm{x}_i^T \bm{\beta}_2$ and $\bm{b}_0 =[b_{01} \hspace{2mm} b_{02}]^T$. Since the the model is bivariate, the full likelihood seems tractable, but we propose below a variable transformation to make the likelihood much simpler without losing any information.

% \begin{equation}
% \begin{multlined}
% \begin{bmatrix} w_{i} \\ v_{i} \end{bmatrix} = 
% \begin{bmatrix} y_{i1} + y_{i2} \\ y_{i1} - y_{i2} \end{bmatrix} = 
% \begin{bmatrix}
%  b_{01} + b_{02} \\ b_{01} - b_{02}
% \end{bmatrix} + 
%  \bm{x}_i^T \begin{bmatrix} \bm{\beta}_1 + \bm{\beta}_2 \\ \bm{\beta}_1 - \bm{\beta}_2\end{bmatrix} + \begin{bmatrix} u_w^2 \\ u_v^2\end{bmatrix}\\
%  \begin{bmatrix} u_w^2 \\ u_v^2\end{bmatrix} \sim 
%  \mathcal{N} \left( \begin{bmatrix}0 \\ 0 \end{bmatrix},
%  \begin{bmatrix} 2 + 2\rho(\bm{x}_i, \bm{\alpha}) & 0 \\ 0 & 2-2\rho(\bm{x}_i, \bm{\alpha}) \end{bmatrix} \right)
%  \end{multlined}
%  \label{transformed_model}
%   \end{equation}

%  The two variables $w_i$ and $v_i$ are independent and the likelihood can be written as a sum of two univariate likelihoods.

% \begin{equation}
% \begin{multlined}
% \ell(\bm{\alpha, b_0, \beta}) = -\frac{N}{2} log(2\pi) - \frac{1}{2} \sum_i log(2 + 2\rho(\bm{x}_i, \bm{\alpha})) - \sum_{i=1}^{N} \frac{(w_i - \mu_w)^2}{2 + 2\rho(\bm{x}_i, \bm{\alpha})} \\
% -\frac{N}{2} log(2\pi) - \frac{1}{2} \sum_i log(2 - 2\rho(\bm{x}_i, \bm{\alpha})) - \sum_{i=1}^{N} \frac{(v_i - \mu_v)^2}{2 - 2\rho(\bm{x}_i, \bm{\alpha})}
% \end{multlined}
% \label{transformed_lik}
% \end{equation}
%  where $\mu_w$ and $\mu_v$ are $b_{01} + b_{02} + \bm{x}_i^T(\bm{\beta}_1 + \bm{\beta}_2)$ and $b_{01} - b_{02} + \bm{x}_i^T(\bm{\beta}_1 - \bm{\beta}_2)$ respectively. This transformation effectively changes the problem from modeling covariance to modeling variance, allowing us to apply results from literature on univariate heteroskedasticity. \\

%  Given the likelihood in (\ref{transformed_lik}), we have two well-known tools to test the null hypothesis (\ref{null1}) --- likelihood ratio test and Rao's score test \footnote{In fact, due to the nuisance parameters, the appropriate terminology is Rao's efficient score test or equivalently Neyman's $C(\alpha)$ test \cite{kocherlakota1991neyman, neyman1979calpha}. Breusch and Pagan (1979) refers to the same procedure as Lagrange Multiplier Test \cite{breusch1979simple}. Bera and Bilias (2001) discusses the historical development of three methods - Rao's score test, Neyman's $C(\alpha)$ test, and Lagrange Multiplier test, and here we choose the term ``score test" as it is most familiar to the statisticians.\cite{bera2001rao}}. Both methods require us to impose certain assumptions on $\rho$, but Rao's score test allows much more flexibility. \\


\subsection{Inference for $K > 2$\label{sec:frameworkK}} 
In section \ref{sec:framework2}, we proposed the test statistic $q$ that tests a pair of variables 1 and 2 to measure the evidence that their correlation changes with respect to the covariate $X$. %After appropriate correction, we have a test statistic that closely follows $\chi_{P}^2$ distribution even when sample size is small. This method can be applied to two genes to test the null hypothesis that their coexpression does not change across genetic ancestry. 
As a natural extension to the pair-wise test statistic, we can repeat the procedure for more than 2 variables. In particular, we can study one transcription factor with  multiple target genes to test whether the local connectivity of the transcription factor varies with genetic ancestry. In this section, we propose a way to combine the test statistics to test a new global null hypothesis. The global null hypothesis for variable $1$ extends \ref{eq:null} as follows,
\begin{equation}
    \bm{H}_0^{(1)}: \bm{\alpha}_{12} = \bm{\alpha}_{13} = \cdots = \bm{\alpha}_{1K} = \bm{0},
\label{eq:globalnull}
\end{equation}
where the superscript in $\bm{H}_0^{(1)}$ indicates that the null hypothesis applies to variable 1. Under $\bm{H}_0^{(1)}$, no other variables' correlation with variable 1 changes across the different values of  $X$. We believe testing the global null \ref{eq:globalnull} improves the statistical power when the ``hot spot" variables or ``hub" variables are connected to a lot of other nodes forming cliques or modules. In the context of gene coexpression network, we know that transcription factors regulate the gene expression of multiple genes, and if one transcription factor varies with respect to the covariate, the transcriptions of those genes regulated by that transcription factor are likely to be correlated with the covariate as well. \\

Combining multiple test statistics can have either positive or negative impact; the procedure can accrue relevant evidence to improve the statistical power, or it can accumulate noise to do the exact opposite. Therefore, we must carefully decide how to combine the test statistics based on the alternative hypothesis we would like to leverage against, and the alternative hypothesis must be constructed to reflect our prior knowledge about the covariance structure. \\

Chen (2012) discusses two ways to construct the alternative hypothesis \cite{chen2012exponential} for testing the global null. One way, called a sparse alternative, is to test whether only a small number among all tests have non-zero effects while all other tests are null. Another way is to test if at least one test has a non-zero effect size. Based on our prior knowledge in biology and coexpression network, we assume that there are many small signals instead of few big ones, so we propose a simpler linear combination of the test statistics
\begin{equation}
d_1 = {q}_{12} + {q}_{13} + \cdots + {q}_{1K} = \sum_{k=2}^{K} {q}_{1k}.
\label{eq:d}
\end{equation}
We believe combining the test statistics like \ref{eq:d} improves the statistical power because even if the effect sizes for each gene pair may be too small to be detected, when combined they can form a stronger signal. \\

We now derive the null distribution of $d_1$ to test \ref{eq:globalnull}. Although ${q}_{1k}$ separately follow $\chi_{P}^2$, they are correlated to one another, so their null distribution is not trivial. We start by re-writing the pair-wise score statistic $q$ (ommitting gene pair index for now) as a sum of $\chi_1^2$ variables as below. Our null hypothesis tests for all covariates at the same time, so we can orthogonalize $X$ to make $\frac{1}{N} \sum_{i=1}^{N} \bm{x}_i \bm{x}_i^T$ an identity matrix without affecting the testing procedure. Let $\tilde{X}$ be the orthogonalized covariate matrix, and $\tilde{x}_{ip}$ be the corresponding entries with $\sum_{i=1}^{N}\tilde{x}_{ip} = 0$ and $\sum_{i=1}^{N} \tilde{x}_{ip}^2  = n$. Then \ref{eq:q} can be alternatively written as follows, where we define $r_{p}.$
\begin{align}
q = \sum_{p=1}^{P}
 \left(\frac{1}{\sqrt{N}}
 \sqrt{\frac{\hat{\sigma}_w^4 \hat{\sigma}_v^4}{\hat{\sigma}_w^4 + \hat{\sigma}_v^4}}
 \sum_{i=1}^{N} x_{ip} \left( \frac{\hat{\sigma}_w^2 - \hat{u}_{wi}^2}{\hat{\sigma}_w^4}
 - \frac{\hat{\sigma}_v^2 - \hat{u}_{vi}^2}{\hat{\sigma}_v^4}
 \right)
\right)^2 = \sum_{p=1}^{P}r_p^2
\label{r}
\end{align}
For each $p$, $r_p$ follows the standard normal distribution by the central limit theorem. 
$$E\left(x_{ip}\left( \frac{\hat{\sigma}_w^2 - \hat{u}_{wi}^2}{\hat{\sigma}_w^4}
 - \frac{\hat{\sigma}_v^2 - \hat{u}_{vi}^2}{\hat{\sigma}_v^4}
 \right)\right) = x_{ip} E\left( \frac{\hat{\sigma}_w^2 - \hat{u}_{wi}^2}{\hat{\sigma}_w^4} \right) E\left( \frac{\hat{\sigma}_v^2 - \hat{u}_{vi}^2}{\hat{\sigma}_v^4}
 \right) = 0$$
$$Var\left(x_{ip}\left( \frac{\hat{\sigma}_w^2 - \hat{u}_{wi}^2}{\hat{\sigma}_w^4}
 - \frac{\hat{\sigma}_v^2 - \hat{u}_{vi}^2}{\hat{\sigma}_v^4}
 \right)\right) = 
 x_{ip}^2 \left( Var\left( \frac{\hat{\sigma}_w^2 - \hat{u}_{wi}^2}{\hat{\sigma}_w^4} \right)  + Var\left( \frac{\hat{\sigma}_v^2 - \hat{u}_{vi}^2}{\hat{\sigma}_v^4}
 \right)\right)
 $$
 $$ = \frac{\hat{\sigma}_w^4 + \hat{\sigma}_v^4}{\hat{\sigma}_w^4 \hat{\sigma}_v^4}$$
This confirms the previous result
$$\sum_{p=1}^{P} r_p^2 \rightarrow \chi_P^2$$
 Now, we acquire a closed-form covariance structure of $r$. First, we begin with a multivariate central limit theorem to write the following in terms of $r$.
\begin{equation}
\bm{r}_{1,p} = \begin{bmatrix}
r_{12,p} \\ r_{13, p} \\ \cdots \\ r_{1K,p}
\end{bmatrix}  \rightarrow N_{K-1}(\bm{0}, H_1) \hspace{5mm} \forall p = 1, \cdots, P
\label{H}
\end{equation}
$H_1$ is a $(K-1) \times (K-1)$ matrix where $(k-1, \ell-1)$th element is $\eta_{1k, 1\ell}$ for $k, \ell = 2, \cdots K$. From \ref{r}, it is easy to see that $H_1$ has 1 at the diagonals. Also, $\eta_{1k, 1\ell}$ converges in probability to
\begin{equation} \begin{multlined}
\frac{(\tau_{23}+2\tau_{12}\tau_{23})(\tau_{12}^2+1)(\tau_{13}^2+1) + \tau_{12}\tau_{13}(6+2(\tau_{12}^2+\tau_{13}^2+\tau_{23}^2))}{(1-{\tau}_{12}^2)(1-{\tau}_{13}^2)\sqrt{(1+{\tau}_{12}^2)(1+{\tau}_{13}^2)}} \\
- \frac{\tau_{12}(\tau_{13}^2+1)(4\tau_{13}+2\tau_{12}\tau_{23}) + \tau_{13}(\tau_{12}^2+1)(4\tau_{12}+2\tau_{13}\tau_{23})}{(1-{\tau}_{12}^2)(1-{\tau}_{13}^2)\sqrt{(1+{\tau}_{12}^2)(1+{\tau}_{13}^2)}}
% + \frac{3\tau_{12}^3\tau_{13}^3 + \tau_{12}^3\tau_{13} + \tau_{12}\tau_{13}^3 - 5\tau_{12}\tau_{13}}{(1-{\tau}_{12}^2)(1-{\tau}_{13}^2)\sqrt{(1+{\tau}_{12}^2)(1+{\tau}_{13}^2)}}
\label{eq:eta}
\end{multlined} \end{equation}
where 
$${\tau}_{k\ell}= \frac{\rho_{k\ell}}{\sqrt{{\rho}_{kk}{\rho}_{\ell\ell}}}. $$
The derivation is in the Appendix. \\

Then, $d_1$ can be written as the sum of L2 norm of $\bm{r}_{1,p}$ with a known distribution, 
\begin{equation}
d_1 = \sum_{p=1}^{P} \|\bm{r}_{1,p}\|_2^2 = \sum_{p=1}^{P} \sum_{k=2}^K r_{1k,p}^2.
\label{d2}
\end{equation}
Let $H_1 = U_1 \Lambda_1 U_1^T$ be the eigen-decomposition of the covariance matrix $H_1$ in \ref{H}, where the diagonal matrix $\Lambda$ has eigenvalues $\lambda_{12}, \cdots, \lambda_{1K}$ in a decreasing order. Then, we can next consider the transformation $\bm{r}_{1,p}^* = U\bm{r}_{1,p}$ that follows normal distribution with diagonal covariance matrix $N_{K-1}(\bm{0}, \Lambda_1)$. Note that $\|\bm{r}_{1,p}\|_2^2 = \|U\bm{r}_{1,p}\|_2^2$ due to the orthogonal invariance of L2 norm. Then,
\begin{equation}
    \begin{multlined}
    \sum_{p=1}^{P} {r_{1k,p}^*}^2 \sim \Gamma \left( \frac{P}{2}, \frac{\lambda_{1k}}{2} \right), \hspace{5mm} k = 2, \cdots, K\\
    d_1 =  \sum_{p=1}^{P} {r_{12,p}^*}^2 + \cdots +\sum_{p=1}^P{r_{1K,p}^*}^2 \sim \sum_{k=2}^{K} \Gamma \left( \frac{P}{2}, \frac{\lambda_{1k}}{2}\right)
    \end{multlined}
    \label{eq:d_dist}
\end{equation}

Assuming that we know the true, symmetric, positive definite $H_1$, we can acquire positive $\lambda_{1k}$ for $k = 2, \cdots, K$, and we have expressed the null distribution of $d_1$ as the sum of distributions of independent gamma variables. We can computationally simulate this null distribution easily. Alternatively, Moschopoulos (1985) provides another interpretation by expressing the cumulative distribution in a form of infinite sum, but the method is inconvenient in practice \cite{moschopoulos1985distribution}.\\

In \ref{eq:eta}, we define the element-wise mapping $\phi: \Sigma \rightarrow H$. It is clear from the construction of $H$ that if we can estimate a well-conditioned, symmetric, positive definite correlation matrix $\hat{\Sigma}$, $\phi(\hat{\Sigma})$ is also symmetric and positive definite. When $N$ is sufficiently larger than $K$, empirical covariance matrix $\hat{\Sigma}$ of $\begin{bmatrix} y_{1i} & y_{2i} & y_{3i} \end{bmatrix}$ is a pretty good consistent estimator for $\Sigma$, and replacing $\Sigma$ with $\hat{\Sigma}$ in computing $H$ can guarantee that the test statistic \ref{d2} converges in distribution to \ref{eq:d_dist}.\\
\begin{theorem}
Consider the global null hypothesis \ref{eq:globalnull} and the test statistic \ref{eq:d}. Under the assumptions listed in Theorem \ref{theorem:q} and an additional assumption that the covariates have been orthogonalized so that $\frac{1}{N} \sum_{i=1}^{N} \bm{x}_i\bm{x}_i^T = I_P$ is $P$ by $P$ identity matrix, $d_1$ asymptotically follows the sum of Gamma distributions as in \ref{eq:d_dist}
\end{theorem}
However, when $K$ is much larger than $n$, an accurate estimation of $\Sigma$ is a difficult problem. We therefore turn to a permutation test and shuffle the covariate vector to test against the true response data that preserves the correlation structure of the network. The permutation procedure is valid under the assumption in \ref{eq:framework_2genes}. \\
% So we instead turn to the permutation test, which is valid under the independence assumption in \ref{eq:framework_2genes}. Empirically, we justify the exchangeability of GTEx subjects through some exploratory analysis which shows that the covariance matrix of the gene expression levels has small non-diagonal elements. Also the principal components didn't show any clustering. We conclude that permutation test is well justified, and so we shuffle the covariate vector and test against the network data to preserve the correlation structure of the network. \\

We use the sequential precision-improvement permutation test, similar to one suggested by Chen (2012) \cite{chen2012exponential}. Permutation test often results in a poor resolution of $p$-values which can lead to imprecise inference especially when we need to correct for the testing of multiple hypotheses. Meanwhile, performing a large number of permutations for many genes can be computationally wasteful. In order to find balance, as we proceed with incrementally larger number of permutations, we count the number of cases that led to more extreme degree statistics than the observed $d_k$. After the minimum number of permutations predefined by the user (1000 in our analysis), if two extreme cases, compared to the true statistic, were found, the permutation procedure is terminated early. If there are less than 2 such cases observed, we perform 100 more permutations and re-check the number of extreme cases and early termination. We repeat until it reaches the predefined maximum number of permutation ($10^6$ in our case), which is designed to give a good enough resolution of $p$-value given the number of tests that we are performing. 

\section{Simulation Studies} \label{simulations}
In this section, we evaluate the proposed method through simulations. We focus on the pairwise analysis and compare the performance of the proposed score test with two other alternatives - liquid association and likelihood ratio test. \\

 First we check the calibration of test statistics under the null hypothesis. We sample $X$ from the univariate standard normal distribution to match the required setting of liquid association. We simulate the data matrix $Y$ from
$$\bm{y}_i \sim \mathcal{N}_2\left(\bm{b}_0 + \bm{x}_i^T\bm{\beta}, \begin{bmatrix} 1 & \bar{\rho} \\ \bar{\rho} & 1 \end{bmatrix}  \right)$$
 where $\bar{\rho}$ was randomly selected from uniform distribution ranging from -1 and 1 and each element of $\bm{b}_0$ and $\bm{\beta}$ from standard normal distribution. We test different sample sizes of $N = 500, 100, 30$ to check the behavior of each method under the null hypothesis. For each $N$, we sample $X$ once, and generate $Y$ 1,000 times. The likelihood ratio test was designed to assume hyperbolic tangent model for $\rho$,
\begin{align}
    \rho(\bm{x}_i^T\bm{\alpha}) = \frac{e^{\bm{x}_i^T\bm{\alpha}}-1}{e^{\bm{x}_i^T\bm{\alpha}}+1},
    \label{eq:data_generating_fisher}
\end{align}
which is the inverse of Fisher transformation, 
$\frac{1}{2} \bm{x}_i^T\bm{\alpha} = \frac{1}{2}log \left(\frac{1+\rho}{1-\rho}\right)$. Fisher-transformed $\rho$ asymptotically follows normal distribution, so it works well when $X$ is drawn from normal distribution. We use \textit{optim} function in R to find $\hat{\bm{\alpha}}_{\text{MLE}}$ under the alternative hypothesis. \\


% Simulate under b_0 = 0 and beta = 0, because estimating MLEs of b_0, beta, and alpha all at the same time is numerically / computationally impossible. Make separate simulation with non-zero mean terms just for the score test.
The results show that all three methods control the type I error at the nominal size well, where score and likelihood ratio test statistics both follow $\chi_1^2$ closely. So we focus on the small sample case $N=30$ as that is close to the size of the real data of African Americans' gene expression level.\\

Next we generate the data under the alternative hypothesis to compare the statistical power. For fixed $N=30$, we again draw $X$ from standard normal distribution. Then for $i = 1, \cdots, N$, we generate $\rho(\bm{x}_i^T\bm{\alpha})$ from hyperbolic tangent function in \ref{eq:data_generating_fisher}. Given $\rho$, we draw $Y$ from \ref{eq1_y} with varying levels of $\alpha$, 1000 times each. The hyperbolic tangent model places the likelihood ratio test at an advantage because the model is correctly specified, so as a contrasting case, we use a quadratic model to generate $\rho$ as follows. Subtracting 0.99 is to ensure numerical stability.

\begin{align}
    \rho(\bm{x}_i^T\bm{\alpha}) = (\bm{x}_i^T\bm{\alpha} - 0.1)^2 - 0.99
    \label{eq:data_generating2_quadratic}
\end{align}

Since the likelihood ratio test assumes a wrong model, it is expected to lose power. Also, since quadratic function is highly non-linear, liquid association is expected to have poor performance as well. Figure \ref{fig:sim} (a) and (b) show the shape of $\rho$ with respect to $X$ with varying levels of $\alpha$. \\
\begin{figure}
     \begin{subfigure}[b]{0.3\textwidth}
         \includegraphics[width=\textwidth]{figures/tanh.png}
         \caption{}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
         \includegraphics[width=\textwidth]{figures/quadratic.png}
         \caption{}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
         \includegraphics[width=\textwidth]{figures/comptime.png}
         \caption{}
     \end{subfigure}
        \caption{\label{fig:sim} (a)-(b) The two heteroskedastic functions $\rho$ used to generate data. Simulation results in Table \ref{pairwise_simulation} show that score test outperforms LA in both cases, and that LA particularly suffers in capturing the non-linear relationship in quadratic model. Likelihood ratio test performs better than score test when the model is correctly specified, but when the model is mis-specified, score test performs better. (c) The computational burden is much lower in the proposed method compared to the other two. LR test requires numerical optimization for finding MLEs, and LA requires permutation test.}
\end{figure}

 Table \ref{pairwise_simulation} summarizes the result. It counts the proportion of simulations which showed $p$-values less than 0.05 out of 1,000 total simulations. When $\rho$ is generated from hyperbolic tangent function, likelihood ratio test generally outperforms the other two methods, as expected since the model is correctly specified in LR test. Score test, although does not assume any model on $\rho$, does not lose as much power as liquid association does. Meanwhile, when $\rho$ is generated from quadratic function, score function clearly outperforms the other two methods. The proposed score test is robust to the shape of heteroskedasticity. Figure \ref{fig:sim} (c) shows the distribution of computation times of each method in the scale of $log_{10}$ for 1000 simulations under quadratic model with $\alpha = 0.5$. The score test is the most efficient, because the likelihood ratio test requires numerical estimation of MLEs both under the null and under the alternative hypothesis while liquid association requires permutation test for inference. 

\begin{table}[!htbp] \centering 
\begin{tabular}{@{\extracolsep{5pt}} cc||cccc||cccc} 
\multicolumn{1}{c}{$\rho$} &\multicolumn{1}{c}{}& \multicolumn{4}{c}{tanh} & \multicolumn{4}{c}{quadratic}\\
\\[-1.8ex]\hline 
\hline \\[-1.8ex]
$\alpha$ & 0 & 0.5 & 1 & 1.5 & 2 & 0.2 & 0.3 & 0.4 & 0.5\\ 
\hline \\[-1.8ex]
score & 0.052 & 0.206 & 0.542 & 0.795 & 0.910 & 0.627 & 0.587 & 0.539 & 0.531\\
LA    & 0.054 & 0.180 & 0.511 & 0.722 & 0.828 & 0.042 & 0047 & 0.058 & 0.079\\
LR    & 0.046 & 0.247 & 0.693 & 0.965 & 0.992 & 0.533 & 0.438 & 0.371 & 0.338\\
\\[-1.8ex]\hline 
\hline \\[-1.8ex]
\end{tabular}
  \caption{ Proportion of simulations for each method that showed $p$-value $<0.05$ at given data generating model and $\alpha$ level. We use two functions for $\rho$, hyperbolic tangent and quadratic function. The likelihood ratio test was conducted under the assumption that $\rho$ is hyperbolic tangent (tanh) function. Proposed score test performs better than liquid association in all cases.  \label{pairwise_simulation}}
\end{table} 


% \subsection{$K>2$}
% First, we simulate data in low dimensional case ($N = 30$ and $K = 5$). This setting allows us to use the test statistic $d$ with null distribution in (\ref{d_dist}). We first compute $H$ using formula in (\ref{eq:eta}) from $\Sigma$ directly assuming $\Sigma$ is known. Then we used $\hat{\Sigma}_{\text{MLE}}$, the empirical covariance matrix to compute $H$. Type I error is controlled well under both cases. \\

%  We also simulate data under the high dimensional case ($N=30$ and $K=1000$) and use permutation test. The computational burden is increases linearly with the number of variables and with the number of permutations. To alleviate the computational burden, we use sequential precision-improvement permutation algorithm, where we start with few permutations $B=100$ and not perform any more if $p$-values is greater than 0.1. Then we perform 1000 permutations for those with $p$-values less than $0.1$, and then move on with only those with $p$ values less than 0.01. We repeat the procedure until we achieve the precision we desire \cite{chen2012exponential}.



% \subsection{Under the Null Hypothesis}


% \subsection{Under the Alternative Hypothesis}

% Power analysis

% \subsection{Comparison with Likelihood Ratio Test}
% A simple alternative to the proposed method is the likelihood ratio test using the model in (\ref{eq2}). In order to conduct the likelihood ratio test, one must assume a specific model in $h$, and estimate the maximum likelihood estimate (MLE) numerically. Explicitly modeling $h$ can either benefit or harm the analysis. If the model is correct, the statistical power will naturally improve; if the model is mis-specified, the performance could be worse. Our proposed method avoids this problem because it is ``model-free" in that the test statistic does not depend on $h$.  In this section, through simulations, we show that our method is better than the likelihood ratio test for three reasons. First, the simulations under the null hypothesis show that the likelihood ratio test statistics are poorly calibrated to $\chi_1^2$ distribution when sample size is not sufficiently large. Second, simulations under the alternative hypothesis show that the statistical power is only minimally better than the proposed method when the model is correctly specified while is often worse than our method when the model is mis-specified. Third, since the maximum likelihood estimate of $\alpha$ cannot be obtained analytically, likelihood ratio test requires numerical optimization to find the MLE for each pair of variables, making it an unattractive choice in terms of computational burden as well.\\

%  The first simulation is under the null hypothesis. We use sample size $n = 30, 100,$ and $500$ and store the $p$-values by comparing the test statistics, both likelihood ratio and Lagrange Multiplier, to the $\chi_1^2$ distribution. Given the sample size $n$, we simulate $X$ once from uniform distribution, to reflect the distribution of genetic ancestry. For this fixed $X$, we simulate $Y_1$ and $Y_2$ 1,000 times from bivariate normal distribution, where $Y_1$ and $Y_2$ both have mean 0 and variance 1 marginally with their covariance $\rho(X)$ fixed to 0.5. \\

%  The results are summarized in Figure \ref{calibration}. The qq-plots shows that the p-values from the likelihood ratio tests are poorly calibrated to the expected uniform distribution for cases of $n=100$ and $n=30$. Meanwhile, the Lagrange Multiplier test, with the help of small sample correction, effectively controls the type I error and the test statistics are well calibrated to $\chi_1^2$ distribution even under the extremely low sample size of $n=30$.

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.98\linewidth]{figures/calibration.png}
%     \caption{\label{calibration} Calibration of $p$-values under the null hypothesis for two methods of Lagrangian Multiplier test and likelihood ratio test. LM stands for Lagrangian Multiplier test, and LR stands for likelihood ratio test.}
% \end{figure}


%  Next, we simulate under the alternative hypothesis where $h$ is specified to be the Fisher transformation defined in (\ref{fisher}). Based on the result of the first simulation, we use sample size $n=500$ to ensure type I error control. Under a simple scenario of $K=2$, we simulate $X$ once from uniform distribution to reflect the distribution of genetic ancestry. For this fixed $X$, we simulate $Y_1$ and $Y_2$ 1,000 times from bivariate normal distribution, where $Y_1$ and $Y_2$ both have mean 0 and variance 1 and their covariance $\rho(X)$ depends on function $h$ as shown in (\ref{sigma}) and (\ref{eq1}). Based on this generated data, we use \textit{optim} function in R to numerically find the MLE of $\alpha$ in model (\ref{eq2}). At first, we conduct the likelihood ratio test assuming correctly that $h$ is Fisher transformation. Then, we conduct the same test with the wrong assumption that $h$ is a cumulative distribution function of standard normal. We finally conduct our proposed ``model-free" method. For all three cases, we record the $p$-values and record how many are smaller than 0.05. \\

%  Table \ref{mle_power} summarizes the statistical power under three scenarios with varying strengths of the signal $\alpha$. The comparison of first and third column shows that when the MLEs were calculated assuming the correct Fisher model, the statistical power is mostly greater than our proposed method, but only marginally. Meanwhile, the comparison of second and third column shows that when the wrong model is assumed, the statistical power is worse than our model-free, flexible method. \\

% \begin{table}[!htbp] \centering 
% \begin{tabular}{lccc} 
% \\[-1.8ex]\hline 
% \hline \\[-1.8ex] 
% & Proposed & Fisher & Normal CDF \\ 
% \hline \\[-1.8ex] 
% $\alpha = 0$ & $0.054$ & $0.051$ & $0.046$ \\ 
% $\alpha = 0.2$ & $0.078$ & $0.084$ & $0.071$  \\ 
% $\alpha = 0.4$ & $0.162$ & $0.172$ & $0.153$ \\ 
% $\alpha = 0.6$ & $0.322$ & $0.338$ & $0.306$  \\ 
% $\alpha = 0.8$ & $0.483$ & $0.490$ & $0.449$ \\ 
% $\alpha = 1$ & $0.628$ & $0.645$ & $0.610$ \\ 
% \hline \\[-1.8ex] 
% \end{tabular} 
%   \caption{  \label{mle_power}  Power analysis under the alternative hypothesis where $h$ is the Fisher transformation in \ref{fisher}. The proposed method doesn't perform as well as the likelihood ratio test when the model is correctly specified, but it performs better than the same test when model is mis-specified. } 
% \end{table}

%  Lastly, we empirically measured the computing time of the likelihood ratio test and the proposed Lagrange Multiplier test. The result summarized in Figure ? suggests that the proposed method is preferred.

% \subsection{Comparison with Liquid Association}

% We also compare the performance of the proposed method with that of liquid association \cite{li2002genome}. Similarly, under the simple scenario of $K=2$, we simulate $X$ once with sample size of $n=100$. (Is this okay? Should I make it 500 for consistency?) For this scenario, we simulate $X$ from standard normal distribution to match the assumptions of liquid association. For this fixed $X$, we simulate $Y_1$ and $Y_2$ 1,000 times from bivariate normal distribution, where $Y_1$ and $Y_2$ both have mean 0 and variance 1 and their covariance $\rho(X)$ depends on function $h$. We use two different $h$ functions; one is Fisher function defined in (\ref{fisher}) which by definition gives a strict range of (0,1) to $\rho$, and the other is a quadratic function designed to do the same for $X$ in $(-2.5, 2.5)$ and $\alpha$ in $(0,4)$.
% $$h(X) = \frac{(X\alpha)^2}{50} + 2.1$$ 
% The two options of $h$ are illustrated in Figure (\ref{la_h}).\\

%  The results in Table \ref{power_la} show that under the Fisher model, the liquid association has higher power, especially as the signal gets stronger. However, when the $\rho(X)$ has a quadratic pattern and is not monotonic with respect to $X$, the same method loses all its statistical power. 

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.4\linewidth]{figures/Fisher.png}
%     \includegraphics[width=0.4\linewidth]{figures/Quadratic.png}
%     \caption{\label{la_h} The two heteroskedastic models used to generate data. Simulation results show that the liquid association can only capture the relationship between $\rho(X)$ and $X$ for the left case.}
% \end{figure}


% \begin{table}[!htbp] \centering 
% \begin{tabular}{@{\extracolsep{5pt}} ccc|cc} 
% \\[-1.8ex]\hline 
% \hline \\[-1.8ex] 
% & \multicolumn{2}{c}{Fisher} & \multicolumn{2}{c}{Quadratic}  \\
% \hline \\[-1.8ex] 
%  & LA & LM & LA & LM \\ 
% \hline \\[-1.8ex] 
% $\alpha=1$ & $0.044$ & $\bm{0.077}$ & $0.005$ & $\bm{0.052}$ \\ 
% $\alpha=2$ & $\bm{0.109}$ & $\bm{0.109}$ & $0.009$ & $\bm{0.052}$ \\ 
% $\alpha=3$ & $\bm{0.150}$ & $0.113$ & $0.005$ & $\bm{0.067}$ \\ 
% $\alpha=4$ & $\bm{0.186}$ & $0.132$ & $0.006$ & $\bm{0.073}$ \\ 
% \hline \\[-1.8ex] 
% \end{tabular} 
%   \caption{True discovery rate for various signal strength $\alpha$ and two models for heterogeneity. LA stands for liquid association and LM stands for Lagrange Multiplier. Under the Fisher model, where the heteroskedasticity is monotonic, liquid association performs better, especially as the signal becomes stronger, because it captures the linear correlation between the covariate and the correlation between the two variables. Under the quadratic model, where heteroskedasticity is non-monotonic, liquid association loses all its power, and the proposed method performs better.   \label{power_la} } 
% \end{table} 


\section{Applications to GTEx Data} \label{applications}
We next apply this method to African American samples from GTEx. We aim to find a group of genes that change its coexpression structure as the genome's proportion of African ancestry changes. The proportion of African ancestry for each individual is defined as global ancestry, and it is referred from software LAMP \cite{pacsaniuc2009imputation}. The data sets are explained in more detail in the Appendix.\\

 We first conduct the data analysis on the supra-pubic skin tissue (not sun exposed), where 31 African American samples are available. Due to low sample size, we restrict our search space to only transcription factors, which are known to have high correlation with many other genes. Therefore, if their impact sizes on other genes are different based on genetic ancestry, such relationship could have important biological implications. \\

\subsection{Data}
 We use the genotype data and normalized gene expression level data from GTEx V6p release \cite{lonsdale2013genotype} to apply the method to the African American samples and their gene coexpression network. The data has been pre-processed by GTEx as explained in the GTEx portal (https://gtexportal.org). In order to select African Americans from the available samples, we first inferred the local ancestry of the samples who identified themselves as European Americans or African Americans and verified that their genetic ancestry is consistent. For local ancestry inference, we use the software LAMP that reaches as high as 98\% accuracy level for distinguishing YRI and CEU ancestry \cite{pacsaniuc2009imputation}. We also need the reference minor allele frequency from the pure population, so we used data from 1000 Genome Project. For the initial setting of hyperparameters in LAMP, we use 7 for the number of generations of admixture, 0.2 and 0.8 for the initial proportion of CEU and YRI population, and $10^{-8}$ for recombination rate, but the results are robust to these settings. LAMP returns local ancestry at each SNP as the count of African chromosomes (0, 1, or 2) at each locus, and we use the SNP closest to the center of the gene to represent the local ancestry of the entire gene.  Around 92\% of the genes show no recombination event in all of the subjects, and less than 3\% of the genes have more than one individual with ancestry switch within the gene, so we believe this is a valid approximation. \\

 We define global ancestry as a value between 0 and 1 that quantifies the proportion of African chromosome in each subject. We first estimate it by averaging the inferred local ancestry, and this estimate is cross-checked with principal component analysis which can effectively cluster the subjects into subpopulations \cite{pritchard2000inference}. We also include pure YRI and CEU population for PCA, and most African Americans lie strictly between the YRI and CEU population showing a two-way admixture between pure Europeans and pure Africans. We observed some outliers that were not placed between pure populations, and so we removed them. We also observed some self-identified Europeans whose genetic ancestry is more than 10\% African, and we include them in our analysis as African Americans. \\

 The expression levels provided by GTEx were measured using RNA-seq for 38,498 genes in the autosomal chromosomes. For each tissue, only genes with RPKM higher than 0.1 were included. Then the expression levels are normalized, log-transformed, and corrected for technical artifacts by GTEx Consortium. \bigskip

We limit our analysis of real data to transcription factors. We acquired a list of transcription factors from TFcheckpoint database \cite{chawla2013tfcheckpoint}. We also acquire a list of target genes for each transcription factors from TF2DNA database \cite{pujato2014prediction}. We only took into consideration target genes with the highest binding scores. \bigskip

\subsection{Analysis}
 For each transcription factor encoding gene $k$, we compute the pair-wise test-statistic $q_{kj}$ for all its target genes $j = 1, \cdots, J_k$. Then, we compute $d_k = \sum_{j=1}^{J_k} q_{kj}$ to test the hypothesis that correlation between the transcription factor $k$ and its targets remain the same across different genetic industry.\bigskip

 We first compare two discrete networks between European Americans and African Americans using binary indicator vector as the covariate $X$, i.e. $x_i = 1$ if subject $i$ is African American, and $x_i =0 $ if subject $i$ is European American. We first compute scores $q_{kj}$ for all the transcription factor encoding gene $k$ and its target genes $j = 1, \cdots, J_k$. Then we sum over all the targets for each transcription factor to compute $d_k$. Then we divide the sum with the number of targets to make a heuristic comparison against $\chi_1^2$ distribution. This procedure essentially observes the average score of all the target genes for a given transcription factor encoding gene, and under the null, the expectation is 1, although the variance is not trivial due to high dependence. Then we choose the top 10 genes with the highest score to further analyze them using the permutation test.\bigskip

 We repeat the same procedure for the African American samples, where $X$ is the vector of global ancestry ranging from 0.3 to 0.95. 

\subsection{Results}
Table \ref{tab:results} summarizes the top 20 transcription factors with the highest $d_k$ values with their $p$-values computed from sequential permutation tests for two cases: (1) comparing two discrete networks, one of African Americans and the other of European Americans, (2) comparing continuously varying coexpression network among African Americans with respect to their global ancestry.
\begin{table}[!htbp] \centering 
\begin{tabular}{@{\extracolsep{5pt}} ccc | ccc} 
    \multicolumn{3}{c||}{AA vs EA} & \multicolumn{3}{c}{AA only}\\
\\[-1.8ex]\hline 
\hline \\[-1.8ex]
genes & $p$-value & tissue & genes & $p$-value & tissue \\ 
\hline \\[-1.8ex] 
ZNF474 & $<10^{-6}$ & adrenal gland & ZBTB20 & 1.622$\cdot 10^{-5}$ & adipose subcutaneous \\ 
HOXA4 & $<10^{-6}$ & artery coronary & KLF7 & 3.461$\cdot 10^{-5}$ & adipose subcutaneous \\ 
SP5 & $<10^{-6}$ & artery coronary& ISL2 & 4.00$\cdot 10^{-5}$ & whole blood \\ 
ZNF638 & $<10^{-6}$ & breast mammary tissue & ZNF285 & 6.579$\cdot 10^{-5}$ & heart atrial appendage  \\ 
MEF2C & $<10^{-6}$ & cells transformed fibroblasts & E2F5 & 8.00$\cdot 10^{-5}$ & adrenal gland \\ 
HMBOX1 & $<10^{-6}$ & cells transformed fibroblasts & SREBF1 & 8.55$\cdot 10^{-5}$ & nerve tibial \\  
TFCP2L1 & $<10^{-6}$ & esophagus mucosa & ZKSCAN3 & 8.62$\cdot 10^{-5}$ & heart atrial appendage \\ 
SMARCC2 & $<10^{-6}$ & esophagus mucosa & NR4A2 & 9.00$\cdot 10^{-5}$ & whole blood \\ 
TFDP2 & $<10^{-6}$ & esophagus muscularis & ZNF682 & 1.29$\cdot 10^{-4}$ & adipose subcutaneous \\ 
DZIP1L & $<10^{-6}$ & lung & TFAP2E & 1.44$\cdot 10^{-4}$ & adrenal gland \\ 
TAX1BP1 & $<10^{-6}$ & stomach & SMAD7 & 1.48$\cdot 10^{-4}$ & artery tibial \\ 
ZFHX4 & $<10^{-6}$ & testis & ZSCAN4 & 1.65$\cdot 10^{-4}$ & muscle skeletal \\ 
ZBTB20 & 2.28$\cdot10^{-5}$ & adipose subcutaneous & ZNF528 & 1.90$\cdot 10^{-4}$ & skin not sun exposed suprapubic \\ 
HOXB7 & 2.53$\cdot10^{-5}$ & artery coronary & HOXB6 & 2.30$\cdot 10^{-4}$ & esophagus muscularis \\ 
ZSCAN4 & 3.58 $\cdot10^{-5}$ & muscle skeletal & FOXK1 & 2.70$\cdot 10^{-4}$ & cells ebv-transformed lymphocytes \\ 
ZSCAN9 & 6.19 $\cdot10^{-5}$ & muscle skeletal & ZNF440 & 2.86$\cdot 10^{-4}$ & skin sun exposed lower leg \\ 
ZNF799 & 6.43 $\cdot10^{-5}$ & testis & HOXB6 & 3.03$\cdot 10^{-4}$ & artery coronary \\ 
ZKSCAN3 & 8.37 $\cdot10^{-5}$ & heart atrial appendage & PLAGL2 & 3.17$\cdot 10^{-4}$ & artery tibial \\
SMAD7 & 1.19 $\cdot10^{-4}$ & artery tibial & MYB & 3.57$\cdot 10^{-4}$ & nerve tibial \\ 
POU6F2 & 1.14$\cdot 10^{-4}$ & testis & POU6F2 & 3.92$\cdot 10^{-4}$ & testis\\
\hline \\[-1.8ex] 
\end{tabular} 
\caption{\label{tab:results}}
\end{table} 

\section{Discussion}
\begin{enumerate}
        \item \textit{Summary}
        \item \textit{Why we don't impose structure on $\Sigma$}
        \item \textit{Limitations: (1) Bivariate normal, (2) For cases $K>n>2$, the burden of permutation test and difficult inference (3) Modeling matrix instead of covariance}
        \item In some cases, the underlying heteroskedasticity could be more complex than what is formulated in \ref{rho}, such as cases where there are interactions among the covariates. 
        \item \textit{Applications outside coexpression network (social network, etc.)}
\end{enumerate}
 The coexpression network is often considered sparse, meaning most of the non-diagonal entries of $\Sigma$ are zeros. However, when we dynamically model the network to vary across the continuous genetic ancestry, the sparsity assumption becomes highly restrictive. Such assumption would require the coexpression between two genes is 0 across all possible genetic ancestry for most genes. In contrast, we believe it is plausible that genes are independent in some ancestry while highly correlated in some other ancestry. More generally, we lack prior information how the coexpression matrix would change across ancestry, and we are open to find any possible patterns. Therefore, we do not assume any specific structure on $\Sigma$. It is difficult to model a continuously varying $K \times K$ covariance matrix within the space of symmetric, positive definite space. Therefore, we first focus on a pair of two variables $1$ and $2$ and model their correlation $\rho(\bm{x}_i)$ in section \ref{pairwise}. Then in section \ref{degree}, we combine information across multiple pairs of genes to make inference on gene's connectivity across the entire network, discussed in detail.

 
 \pagebreak
 
\section{Appendix}
\subsection{Derivation of $q$}
We start from the model in \ref{eq:framework_2genes}. The hat notation is used to note the maximum likelihood estimates. The nuisance parameters can be replaced with their respective maximum likelihood estimators under the null hypothesis. When $\bm{\alpha} = \bm{0}$, $\hat{\bm{\beta}}$ and $\hat{\bm{b}}$ can be acquired from ordinary linear squares estimates. The variance terms $\hat{\rho}_{11}$ and $\hat{\rho}_{22}$ are
$$\hat{\rho}_{11} =  \frac{\sum_{i=1}^{N}(y_{i1} - b_1 - \bm{x}_i^T\bm{\hat{\beta}}_{1})^2}{N}$$
$$\hat{\rho}_{22}=  \frac{\sum_{i=1}^{N}(y_{i2} - b_2 - \bm{x}_i^T\bm{\hat{\beta}}_{2})^2}{N}$$

Then we look at the likelihood after the variable transformation in \ref{eq:transformed_2genes}.
\begin{equation}
\begin{multlined}
\ell(\bm{\alpha}, \bm{b}, \bm{\beta}, \rho_{11}, \rho_{22}) = \\
-\frac{N}{2} log(2\pi) - \frac{1}{2} \sum_{i=1}^{N} log (\sigma_{wi}^2) - \sum_{i=1}^{N} \frac{(w_i - \mu_{wi})^2}{\sigma_{wi}^2}\\
\hspace{7mm} -\frac{N}{2} log(2\pi) - \frac{1}{2} \sum_{i=1}^{N} log (\sigma_{vi}^2) - \sum_{i=1}^{N} \frac{(v_i - \mu_{vi})^2}{\sigma_{vi}^2}
\end{multlined}
    \label{eq:likelihood_2genes_appendix}
\end{equation}
$$\mu_{wi} = \frac{{b}_1}{\sqrt{\rho_{11}}} + \frac{{b}_2}{\sqrt{\rho_{22}}} + \bm{x}_i^T \left(\frac{\bm{\beta}_1}{\sqrt{\rho_{11}}} + \frac{\bm{\beta}_2}{\sqrt{\rho_{22}}}\right), \hspace{6mm} 
\mu_{vi} = \frac{{b}_1}{\sqrt{\rho_{11}}} - \frac{{b}_2}{\sqrt{\rho_{22}}} + \bm{x}_i^T \left(\frac{\bm{\beta}_1}{\sqrt{\rho_{11}}} - \frac{\bm{\beta}_2}{\sqrt{\rho_{22}}}\right)$$
$$\sigma_{wi}^2 = 2 + \frac{2\rho_{12}(\bm{x}_i^T \bm{\alpha})}{\sqrt{\rho_{11}\rho_{22}}}, \hspace{6mm} \sigma_{vi}^2 = 2 - \frac{2\rho_{12}(\bm{x}_i^T \bm{\alpha})}{\sqrt{\rho_{11}\rho_{22}}}$$

Then we compute the first and second derivative of the log likelihood with respect to $\bm{\alpha}$ when all other nuisance parameters are replaced with their respective maximum likelihood estimates. The following results help the computation of the derivative.
$$
\frac{\partial \sigma_{wi}^2}{\partial \bm{\alpha}} = \frac{2\bm{x}_i \rho_{12}'(\bm{x}_i^T \bm{\alpha})}{\sqrt{\rho_{11}\rho_{22}}}, \hspace{5mm}
\frac{\partial \sigma_{vi}^2}{\partial \bm{\alpha}} = -\frac{2\bm{x}_i \rho_{12}'(\bm{x}_i^T \bm{\alpha})}{\sqrt{\rho_{11}\rho_{22}}}
$$
$$\frac{\partial \sigma_{wi}^2}{\partial \rho_{11}} = -\frac{\rho_{12}(\bm{x}_i^T \bm{\alpha})}{\sqrt{\rho_{11}^3\rho_{22}}}, \hspace{5mm}
\frac{\partial \sigma_{vi}^2}{\partial \rho_{11}} = -\frac{\rho_{12}(\bm{x}_i^T \bm{\alpha})}{\sqrt{\rho_{11}\rho_{22}^3}}$$

\noindent We additionally define $\hat{u}_{wi}^2$ and $\hat{v}_{wi}^2$ as the residuals from the linear regression: $(w_i - \hat{\mu}_{wi})^2$ and $(v_i-\hat{\mu}_{vi})^2$ respectively, where $\hat{\mu}_{wi}$ and $\hat{\mu}_{vi}$ are $\mu_{wi}$ and $\mu_{vi}$ with $\bm{b}$ and $\bm{\beta}$ replaced with $\hat{\bm{b}}$ and $\hat{\bm{\beta}}$.\\

\noindent The maximum likelihood estimates for $\sigma_{wi}^2$ and $\sigma_{vi}^2$ are as follows.
$$\hat{\sigma}_{wi}^2 = \frac{1}{N} \sum_{i=1}^{N} \hat{u}_{wi}^2, \hspace{5mm} 
\hat{\sigma}_{wi}^2 = \frac{1}{N} \sum_{i=1}^{N} \hat{u}_{vi}^2$$

\begin{equation}
    \begin{multlined}
    \bm{d}_{\bm{\alpha}} = \frac{\partial \ell}{\partial \bm{\alpha}} = - \sum_{i=1}^{N} \frac{1}{2\sigma_{wi}^2} \cdot \frac{\partial \sigma_{wi}^2}{\partial \bm{\alpha}} - \sum_{i=1}^{N} \frac{-(w_i - \mu_{wi})^2}{2\sigma_{wi}^4} \frac{\partial \sigma_{wi}^2}{\partial \bm{\alpha}}\\
    - \sum_{i=1}^{N} \frac{1}{2\sigma_{vi}^2} \cdot \frac{\partial \sigma_{vi}^2}{\partial \bm{\alpha}} - \sum_{i=1}^{N} \frac{-(v_i - \mu_{vi})^2}{2\sigma_{vi}^4} \frac{\partial \sigma_{vi}^2}{\partial \bm{\alpha}}\\
    =-\frac{1}{2\sqrt{\rho_{11}\rho_{22}}} \sum_{i=1}^{N} \bm{x}_i \rho_{12}'(\bm{x}_i^T\bm{\alpha}) \cdot \left(
    \frac{1}{\sigma_{wi}^2} - \frac{u_{wi}^2}{\sigma_{wi}^4} 
    - \frac{1}{\sigma_{vi}^2} + \frac{u_{vi}^2}{\sigma_{vi}^4}
    \right)\\
    = -\frac{1}{2\sqrt{\rho_{11}\rho_{22}}} \sum_{i=1}^{N} \bm{x}_i\rho_{12}'(\bm{x}_i^T \bm{\alpha}) \cdot 
    \left(
    \frac{1}{\sigma_{wi}^2} \left( 1 - \frac{u_{wi}^2}{\sigma_{wi}^2}\right) - 
    \frac{1}{\sigma_{vi}^2} \left( 1 - \frac{u_{vi}^2}{\sigma_{vi}^2} \right)
    \right)
    \end{multlined}
\end{equation}

Plugging in $\bm{0}$ for $\bm{\alpha}$ and maximum likelihood estimators for all nuisance parameters, we get the score function as below. 
\begin{equation}
    \begin{multlined}
    \bm{\tilde{d}}_{\bm{\alpha}} = \bm{d}_{\bm{\alpha}}\mid_{\bm{\alpha}=\bm{0}, \bm{b} = \bm{\hat{b}},\bm{\beta} = \bm{\hat{\beta}}}
    {\rho}_{11} = {\bm{\hat{\rho}}_{11}},
    {\rho}_{22} = {\bm{\hat{\rho}}_{22}}\\
    = -\frac{\rho_{12}'(\bm{0})}{2\sqrt{\rho_{11}\rho_{22}}} \sum_{i=1}^{N} \frac{\bm{x}_i}{\hat{\sigma}_{w}^2} - \frac{\bm{x}_i}{\hat{\sigma}_{v}^2} - \frac{\bm{x}_i \hat{u}_{wi}^2}{\hat{\sigma}_{wi}^4} + \frac{\bm{x}_i \hat{u}_{vi}^2}{\hat{\sigma}_{vi}^4}\\
    = -\frac{\rho_{12}'(\bm{0})}{2\sqrt{\rho_{11}\rho_{22}}} \sum_{i=1}^{N} \left(
    \frac{\bm{x}_i}{\hat{\sigma}_{w}^2}
    \left( 1-\frac{\hat{u}_{wi}^2}{\hat{\sigma}_{w}^2}\right) - 
    \frac{\bm{x}_i}{\hat{\sigma}_{v}^2}
    \left( 1-\frac{\hat{u}_{vi}^2}{\hat{\sigma}_{v}^2}\right) 
    \right)
    \end{multlined}
\end{equation}

% \begin{align*}
% \bm{d}_{\bm{\alpha}} &= \frac{\partial \ell}{\partial \bm{\alpha}}\\
% &= -\frac{1}{2} \sum_{i=1}^{N}  \frac{2\rho'(\bm{x}_i^T \bm{\alpha})\bm{x}_i}{\sigma_{wi}^2} \left(
% 1 - \frac{u_{wi}^2}{\sigma_{wi}^2} 
% \right) + \frac{1}{2} \sum_{i=1}^{N}\frac{2\rho'(\bm{x}_i^T \bm{\alpha})\bm{x}_i}{\sigma_{vi}^2} \left(
% 1 - \frac{u_{vi}^2}{\sigma_{vi}^2}\right).
% \end{align*}

%  Now, we replace $\bm{\beta}$ and $\bm{\alpha}$ with the maximum likelihood estimators under the null hypothesis. $\bm{\beta}$ is therefore replaced by the OLS estimators, and $\bm{\alpha}$ is 0. Moreover, the MLE for
% $2 + 2\rho(\bm{0}) = \hat{\sigma}_w^2$ and MLE for $2-2\rho(\bm{0}) = \hat{\sigma}_v^2$. 
% \begin{equation}
%     \begin{multlined}
% \bm{\tilde{d}_{\alpha}} = \bm{d}_{\bm{\alpha}}\mid_{\bm{\alpha}=\bm{0}, \bm{\beta} = \bm{\hat{\beta}}_{\text{MLE}}} \\
% = -\frac{1}{2} \sum_i \left(
% \frac{2\rho'(0)\bm{x}_i}{\hat{\sigma}_{w}^2} \left(
% 1-\frac{\hat{u}_{wi}^2}{\hat{\sigma}_{w}^2}
% \right)
% -
% \frac{2\rho'(0)\bm{x}_i}{\hat{\sigma}_{v}^2} \left(
% 1-\frac{\hat{u}_{vi}^2}{\hat{\sigma}_{v}^2}
% \right)
% \right)\\
% = -\rho'(0) \sum_i  \left(
% \frac{\bm{x}_i}{\hat{\sigma}_{w}^2} \left(
% 1-\frac{\hat{u}_{wi}^2}{\hat{\sigma}_{w}^2}
% \right)
% -
% \frac{\bm{x}_i}{\hat{\sigma}_{v}^2} \left(
% 1-\frac{\hat{u}_{vi}^2 }{\hat{\sigma}_{v}^2}
% \right)
% \right)
% \label{first_derivative}
% \end{multlined}
% \end{equation}

For the second derivative, 
\begin{equation}
    \begin{multlined}
    \frac{\partial^2 \ell}{\partial \bm{\alpha} \partial \bm{\alpha}^T}
    = \sum_{i=1}^{N} \frac{\bm{x}_i \bm{x}_i^T \rho_{12}''(\bm{x}_i^T\bm{\alpha})}{2\sqrt{\rho_{11}\rho_{22}}} \cdot
    \left(
    \frac{1}{\sigma_{wi}^2} \left( 1 - \frac{u_{wi}^2}{\sigma_{wi}^2}\right) - 
    \frac{1}{\sigma_{vi}^2} \left( 1 - \frac{u_{vi}^2}{\sigma_{vi}^2} \right)
    \right)\\
    + \frac{\bm{x}_i \bm{x}_i^T {\rho_{12}'}^2(\bm{x}_i^T\bm{\alpha})}{4\rho_{11}\rho_{22}}
    \left(
    -\frac{1}{\sigma_{wi}^4} + \frac{u_{wi}^2}{2\sigma_{wi}^6} + 
    \frac{1}{\sigma_{vi}^4} + \frac{u_{vi}^2}{2\sigma_{vi}^6}
    \right)\\
    = \sum_{i=1}^{N} \frac{\bm{x}_i \bm{x}_i^T \rho_{12}''(\bm{x}_i^T\bm{\alpha})}{2\sqrt{\rho_{11}\rho_{22}}} \cdot
    \left(
    \frac{1}{\sigma_{wi}^2} \left( 1 - \frac{u_{wi}^2}{\sigma_{wi}^2}\right) - 
    \frac{1}{\sigma_{vi}^2} \left( 1 - \frac{u_{vi}^2}{\sigma_{vi}^2} \right)
    \right)\\
    + \frac{\bm{x}_i \bm{x}_i^T {\rho_{12}'}^2(\bm{x}_i^T\bm{\alpha})}{4\rho_{11}\rho_{22}}
    \left(
    \frac{2}{\sigma_{wi}^4} \left(1-\frac{u_{wi}^2}{2\sigma_{wi}^2}\right)+
    \frac{2}{\sigma_{vi}^4} \left(1-\frac{u_{vi}^2}{2\sigma_{vi}^2}\right)
    \right)
    \end{multlined}
\end{equation}

\begin{equation}
    \mathcal{I}_{\bm{\alpha}\bm{\alpha}^T} = E\left(\frac{\partial^2 \ell}{\partial \bm{\alpha} \partial \bm{\alpha}^T}\right)\\
    = \sum_{i=1}^{N} \frac{\bm{x}_i \bm{x}_i^T {\rho '}^2_{12}(\bm{x}_i^T\bm{\alpha})}{4\rho_{11}\rho_{22}} \left(
    \frac{1}{\sigma_{wi}^4} + \frac{1}{\sigma_{vi}^4}
    \right)
\end{equation}

Plug in the maximum likelihood estimates:
\begin{equation}
\begin{multlined}
        \tilde{\mathcal{I}}_{\bm{\alpha}\bm{\alpha}^T} = \mathcal{I}_{\bm{\alpha}\bm{\alpha}^T}\mid_{
        \bm{\alpha}=\bm{0}, 
        \bm{b} = \bm{\hat{b}},
        \bm{\beta} = \bm{\hat{\beta}},
        {\rho}_{11} = \bm{\hat{\rho}}_{11},
        {\rho}_{22} = \bm{\hat{\rho}}_{22}}\\
        = \frac{{\rho '}^2_{12}(\bm{0})}{4\hat{\rho}_{11}\hat{\rho}_{22}}\cdot 2 \cdot \left(\frac{1}{\hat{\sigma}_{w}^2} + \frac{1}{\hat{\sigma}_{v}^2} \right) \sum_{i=1}^{N} \bm{x}_i \bm{x}_i^T
    \end{multlined}
\end{equation}

Additionally, we can see that $\tilde{\mathcal{I}}_{\bm{\alpha}{\bm{\beta}_1^T}} =  \tilde{\mathcal{I}}_{\bm{\alpha}{\bm{\beta}_2^T}} = \bm{0}$
\begin{equation}
\frac{\partial \ell}{\partial \bm{\alpha} \partial\bm{\beta}_1}= 
    % {\mathcal{I}}_{\bm{\alpha}{\bm{\beta}_1^T}} =
    \frac{1}{\sqrt{\rho_{11}\rho_{22}}} \sum_i \bm{x}_i \bm{x}_i^T \rho_{12}(\bm{x}_i^T \bm{\alpha}) \left( 
    \frac{u_{wi}}{\sqrt{\rho_{11}}\sigma_{wi}^4} + \frac{u_{vi}}{\sqrt{\rho_{11}}\sigma_{vi}^4}
    \right)
\end{equation}
$$
\tilde{\mathcal{I}}_{\bm{\alpha}{\bm{\beta}_1^T}} = 
\frac{\rho_{12}(\bm{0})}{\sqrt{\rho_{11}^3\rho_{22}}} \left(
\frac{1}{\hat{\sigma}_{wi}^4}
E\left( \sum_{i=1}^{n} \bm{x}_i \bm{x}_i^T \hat{u}_{wi}\right)
+\frac{1}{\hat{\sigma}_{vi}^4}
E\left( \sum_{i=1}^{n} \bm{x}_i \bm{x}_i^T \hat{u}_{vi} \right)
\right) = \bm{0}$$

\noindent $\tilde{\mathcal{I}}_{\bm{\alpha}\bm{\rho}_{11}}$ and $\tilde{\mathcal{I}}_{\bm{\alpha}\bm{\rho}_{22}}$ are as follows.
\begin{equation}
    \begin{multlined}
    \frac{\partial \ell}{\partial \bm{\alpha}\partial\rho_{11}} = \\
    %  {\mathcal{I}}_{\bm{\alpha}\bm{\rho}_{11}}= \\
     \sum_{i=1}^{N} -\frac{\bm{x}_i \rho'_{12}(\bm{x}_i^T \bm{\alpha})}{2\sqrt{\rho_{11}^3\rho_{22}}} \left( \frac{1}{\sigma_{wi}^2}\left( 1-\frac{u_{wi}^2}{\sigma_{wi}^2} \right) - 
     \frac{1}{\sigma_{vi}^2} \left( 1-\frac{u_{vi}^2}{\sigma_{vi}^2}\right) \right)\\
     + \frac{\bm{x}_i \rho_{12}(\bm{x}_i^T \bm{\alpha}) \rho_{12}'(\bm{x}_i^T \bm{\alpha})}{2\rho_{11}^2 \rho_{22}}\left( \frac{1}{\sigma_{wi}^4}\left( 1-\frac{u_{wi}^2}{2\sigma_{wi}^2} \right) - 
     \frac{1}{\sigma_{vi}^4} \left( 1-\frac{u_{vi}^2}{2\sigma_{vi}^2}\right) \right)
    \end{multlined}
\end{equation}

\begin{equation*}
\tilde{\mathcal{I}}_{\bm{\alpha}\bm{\rho}_{11}} = \frac{\rho_{12}(\bm{0})\rho_{12}'(\bm{0})}{2\rho_{11}^2 \rho_{22}^2} \sum_{i=1}^{N} \bm{x}_i \left( \frac{1}{2\hat{\sigma}_w^4} + \frac{1}{2\hat{\sigma}_v^4}\right)
\end{equation*}

We can see that above equals to zero under a small condition that the covariates are centered. The centering of covariates play no role in inference. \\

Finally, we get our score statistic
\begin{equation}
    \begin{multlined}
    q = \tilde{d}_{\bm{\alpha}}^T \tilde{I}_{\bm{\alpha}\bm{\alpha}^T}^{-1}
    \tilde{d}_{\bm{\alpha}}
    =\frac{1}{2}\left( \frac{1}{\hat{\sigma}_w^4}+\frac{1}{\hat{\sigma}_v^4}\right)^{-1}
\left(\sum_{i=1}^{N} \bm{x}_{i} 
\left(
\frac{\hat{\sigma}_w^2-\hat{u}_{wi}^2}{\hat{\sigma}_w^4}- \frac{\hat{\sigma}_v^2-\hat{u}_{vi}^2}{\hat{\sigma}_v^4}
\right)
\right)^T \\
\left( \sum_{i=1}^{N} \bm{x}_i^T \bm{x}_i \right)^{-1}
\left(
\sum_{i=1}^{N} \bm{x}_{i} 
\left(
\frac{\hat{\sigma}_w^2-\hat{u}_{wi}^2}{\hat{\sigma}_w^4}- \frac{\hat{\sigma}_v^2-\hat{u}_{vi}^2}{\hat{\sigma}_v^4}
\right)
\right)
    \end{multlined}
\end{equation}

\pagebreak


% \begin{align*}
% I_{\bm{\alpha\alpha^T}} &= \frac{\partial^2 \ell}{\partial \bm{\alpha} \partial \bm{\alpha^T}} \\
% &= 
% -\frac{1}{2} \frac{\partial}{\partial {\bm{\alpha}}} 
% \sum_{i=1}^{N} \bm{x}_i\frac{2\rho'(\bm{x}_i^T\bm{\alpha})}{\sigma_{wi}^2}\left( 1- \frac{{u}_{vi}^2}{\sigma_{wi}^2}\right) + \frac{1}{2} \sum_{i=1}^{N} \bm{x}_i \frac{-2\rho'(\bm{x}_i^T\bm{\alpha})}{2-2\rho(\bm{x}_i^T\bm{\alpha})} \left(1 - \frac{{u}_{vi}^2}{2-2\rho(\bm{x}_i^T\bm{\alpha})} \right)\\
% &= -\frac{1}{2} \sum_{i=1}^{N} \bm{x}_i \bm{x}_i^T \bigg(\frac{(\sigma_{wi}^2) (\sigma_{wi}^2)-{u}_{wi}^2) 2\rho''(\bm{x}_i^T\bm{\alpha}) + (2{u}_{wi}^2 - (\sigma_{wi}^2))(2\rho'(\bm{x}_i^T\bm{\alpha}))^2}{(\sigma_{wi}^2)^3} \\
% & \hspace{8mm} - \frac{(2-2\rho(\bm{x}_i^T\bm{\alpha})) (2-2\rho(\bm{x}_i^T\bm{\alpha})-{u}_{vi}^2) (-2\rho''(\bm{x}_i^T\bm{\alpha})) + (2{u}_{vi}^2 - (2-2\rho(\bm{x}_i^T\bm{\alpha})))(-2\rho'(\bm{x}_i^T\bm{\alpha}))^2}{(2-2\rho(\bm{x}_i^T\bm{\alpha}))^3}  \bigg)\\
% & = -\frac{1}{2} \sum_{i=1}^{N} \bm{x}_i \bm{x}_i^T\left(
% 2\rho''(\bm{x}_i^T\bm{\alpha}) \left( \frac{(2+2\rho(\bm{x}_i^T\bm{\alpha}))(2+2\rho(\bm{x}_i^T\bm{\alpha})-u_{wi}^2)}{(2+2\rho(\bm{x}_i^T\bm{\alpha}))^3} + \frac{(2-2\rho(\bm{x}_i^T\bm{\alpha}))(2-2\rho(\bm{x}_i^T\bm{\alpha})-u_{vi}^2)}{(2-2\rho(\bm{x}_i^T\bm{\alpha}))^3} \right) \\
% & \hspace{8mm} +2\rho'(\bm{x}_i^T\bm{\alpha})^2 \left( \frac{2u_{wi}^2 - (2+2\rho(\bm{x}_i^T\bm{\alpha}))}{(2+2\rho(\bm{x}_i^T\bm{\alpha}))^3} + \frac{2u_{vi}^2 - (2-2\rho(\bm{x}_i^T\bm{\alpha}))}{(2-2\rho(\bm{x}_i^T\bm{\alpha}))^3}\right)
% \right)
% \end{align*}
% Plugging in $\bm{\alpha} = \bm{0}$ and $\bm{\beta} = \hat{\bm{\beta}}_{\text{MLE}}$, above becomes
% \begin{align*}
% &\tilde{I}_{\bm{\alpha\alpha^T}} = 
% {I}_{\bm{\alpha\alpha}}\mid_{\bm{\alpha}=\bm{0}, \bm{\beta} = \bm{\hat{\beta}}_{\text{MLE}}}\\
% &= -\frac{1}{2} \sum_{i=1}^{N} \bm{x_i x_i}^T\bigg(
% \frac{\hat{\sigma}_w^2 (\hat{\sigma}_w^2 - \hat{u}^2_{wi}) 2\rho''(0)  + (2\hat{u}_{wi}^2 - \hat{\sigma}_w^2)(2\rho'(0))^2}{\hat{\sigma}_w^6} - \frac{\hat{\sigma}_v^2(\hat{\sigma}_v^2 - \hat{u}_{vi}^2)2\rho''(0) + (2\hat{u}_{vi}^2 - \hat{\sigma}_v^2)(2\rho'(0))^2}{\hat{\sigma}_v^6}
% \bigg)\\
% &= -{\rho''(0)}\sum_{i=1}^{N}\bm{x_ix_i}^T \bigg(
% \frac{\hat{\sigma}_w^2(\hat{\sigma}_w^2 - \hat{u}_{wi}^2)}{\hat{\sigma}_w^6} - \frac{\hat{\sigma}_v^2(\hat{\sigma}_v^2 - \hat{u}_{vi}^2)}{\hat{\sigma}_v^6}
% \bigg) - 2\rho'(0)^2 \sum_{i=1}^{N} \bm{x_i^Tx_i} \bigg( 
% \frac{2\hat{u}_{wi}^2 - \hat{\sigma}_w^2}{\hat{\sigma}_w^6} - \frac{2\hat{u}_{vi}^2 - \hat{\sigma}_v^2}{\hat{\sigma}_v^6}
% \bigg)
% \end{align*}
% Since $\hat{\sigma}_w^2 = \frac{1}{N} \sum_{i=1}^{N} \hat{u}_{wi}^2$, 
% $$\sum_{i=1}^{N} \bm{x_ix_i}^T\frac{\hat{\sigma}_w^2(\hat{\sigma}_w^2-\hat{u}_{wi}^2)}{\hat{\sigma}_w^6}
%  =\frac{1}{\hat{\sigma}_w^4} \sum_{i=1}^{N}  \bigg( 
% \frac{1}{N} \bm{x_i^Tx_i} \sum_{j=1}^{N} {\hat{u}_{wj}^2}  - \bm{x_i^Tx_i} \hat{u}_{wi}^2
%  \bigg) = 0 $$
%  and similarly,
%  $$\sum_{i=1}^{N} \bm{x_i^Tx_i} \frac{\hat{\sigma}_v^2(\hat{\sigma}_v^2 - \hat{u}_{vi}^2)}{\hat{\sigma}_v^6} = 0.$$
%  Therefore, 
% \begin{align*}
% \tilde{I}_{\bm{\alpha\alpha}} &= {I}_{\bm{\alpha\alpha}}\mid_{\bm{\alpha}=\bm{0}, \bm{\beta} = \bm{\hat{\beta}}_{\text{MLE}}} \\
% &= 2\rho'(0)^2 \left( \frac{1}{\hat{\sigma}_w^4}  + \frac{1}{\hat{\sigma}_v^4}\right)\sum_{i=1}^{N}  \bm{x}_i \bm{x}_i^T
% \end{align*}

%  Therefore, the test statistic $q$ is 
% \begin{equation}
% q = \tilde{d}_{\bm{\alpha}} \tilde{I}_{\bm{\alpha\alpha}}^{-1} \tilde{d}_{\bm{\alpha}}
% \label{original_LM}
% \end{equation}
% $$ = \frac{1}{2} \cdot \frac{1}{\frac{1}{\hat{\sigma}_w^4} + \frac{1}{\hat{\sigma}_v^4}} \left(
% \sum_{i=1}^{N} \bm{x_{i}} 
% \left(
% \frac{1}{\hat{\sigma}_w^2} - \frac{\hat{u}_{wi}^2}{\hat{\sigma}_w^4}\right)  - \left( \frac{1}{\hat{\sigma}_v^2} - \frac{\hat{u}_{vi}^2}{\hat{\sigma}_v^4}
% \right)
% \right)^T 
% \left( \sum_{i=1}^{N} \bm{x_i^T x_i} \right)^{-1}
% \left(
% \sum_{i=1}^{N} \bm{x_{i}} 
% \left(
% \frac{1}{\hat{\sigma}_w^2} - \frac{\hat{u}_{wi}^2}{\hat{\sigma}_w^4}\right)  - \left( \frac{1}{\hat{\sigma}_v^2} - \frac{\hat{u}_{vi}^2}{\hat{\sigma}_v^4}
% \right)
% \right)
% $$


\subsection{Derivation of $\eta_{12,13}$}
We start from the test statistic $q$ computed when the covariates have been orthogonalized.
$$q = \sum_{p=1}^{P}
 \left(\frac{1}{\sqrt{N}}
 \sqrt{\frac{\hat{\sigma}_w^4 \hat{\sigma}_v^4}{\hat{\sigma}_w^4 + \hat{\sigma}_v^4}}
 \sum_{i=1}^{N} x_{ip} \left( \frac{\hat{\sigma}_w^2 - \hat{u}_{wi}^2}{\hat{\sigma}_w^4}
 - \frac{\hat{\sigma}_v^2 - \hat{u}_{vi}^2}{\hat{\sigma}_v^4}
 \right)
\right)^2 = \sum_{p=1}^{P}r_p^2$$
$r_p$ for each $p$ follows standard normal distribution by the central limit theorem. 
$$E\left(x_{ip}\left( \frac{\hat{\sigma}_w^2 - \hat{u}_{wi}^2}{\hat{\sigma}_w^4}
 - \frac{\hat{\sigma}_v^2 - \hat{u}_{vi}^2}{\hat{\sigma}_v^4}
 \right)\right) = x_{ip} E\left( \frac{\hat{\sigma}_w^2 - \hat{u}_{wi}^2}{\hat{\sigma}_w^4} \right) E\left( \frac{\hat{\sigma}_v^2 - \hat{u}_{vi}^2}{\hat{\sigma}_v^4}
 \right) = 0$$
$$Var\left(x_{ip}\left( \frac{\hat{\sigma}_w^2 - \hat{u}_{wi}^2}{\hat{\sigma}_w^4}
- \frac{\hat{\sigma}_v^2 - \hat{u}_{vi}^2}{\hat{\sigma}_v^4}
\right)\right) = 
x_{ip}^2 \left( Var\left( \frac{\hat{\sigma}_w^2 - \hat{u}_{wi}^2}{\hat{\sigma}_w^4} \right)  + Var\left( \frac{\hat{\sigma}_v^2 - \hat{u}_{vi}^2}{\hat{\sigma}_v^4}\right)\right)$$
 $$ = \frac{\hat{\sigma}_w^4 + \hat{\sigma}_v^4}{\hat{\sigma}_w^4 \hat{\sigma}_v^4}$$

We can re-write $r_{12,p}$ as following as a pre-processing to compute $cov(r_{12,p}, r_{12,p})$,
$$\hat{\sigma}_w^2 = 2 + \frac{2\hat{\rho}_{12}}{\sqrt{\hat{\rho}_{11}\hat{\rho}_{22}}} = 2+2\hat{\tau}_{12}$$
$$\hat{\sigma}_v^2 = 2 - \frac{2\hat{\rho}_{12}}{\sqrt{\hat{\rho}_{11}\hat{\rho}_{22}}} = 2-2\hat{\tau}_{12}$$
where $\rho_{12}$ is the constant correlation between variables 1 and 2 under the null hypothesis. Note that asymptotically $\hat{\sigma}_w^2$ converges to $\sigma_w^2$ in probability, and so does $\hat{\rho}_{12}$ to $\rho_{12}$. We also define
$\hat{\tau}_{13} = \frac{\hat{\rho}_{13}}{\sqrt{\hat{\rho}_{11}\hat{\rho}_{33}}}$
$\hat{\tau}_{23} = \frac{\hat{\rho}_{23}}{\sqrt{\hat{\rho}_{22}\hat{\rho}_{33}}}$
\begin{equation}
\begin{multlined}
    r_{p,12} = \frac{1}{\sqrt{2N}} \frac{1}{\sqrt{8(1 + \hat{\tau}_{12}^2)}}
    \sum_{i=1}^{N} x_{ip}\left( (2-2\hat{\tau}_{12})- \frac{2-2\hat{\tau}_{12}}{2+2\hat{\tau}_{12}}(\hat{u}_{1i}+\hat{u}_{2i})^2\right)\\  - \left( (2+2\hat{\tau}_{12}) - \frac{2+2\hat{\tau}_{12}}{2-2\hat{\tau}_{12}}(\hat{u}_{1i}-\hat{u}_{2i})^2\right)\\
    = \frac{1}{\sqrt{16N(1+\hat{\tau}_{12}^2)}} \sum_{i=1}^{N}x_{ip}
    \left( 
    \frac{4((\hat{\tau}_{12}^3 - \hat{\tau}_{12}) - \hat{u}_{1i}\hat{u}_{2i}(\hat{\tau}_{12}^2 + 1) + \hat{\tau}_{12}(\hat{u}_{1i}^2 + \hat{u}_{2i}^2))}{(1-\hat{\tau}_{12})(1+\hat{\tau}_{12})}
    \right)\\
    = \frac{1}{\sqrt{N(1+\hat{\tau}_{12}^2)}}\sum_{i=1}^{N}x_{ip} \left( 
    \frac{(\hat{\tau}_{12}^3 - \hat{\tau}_{12}) - \hat{u}_{1i}\hat{u}_{2i}(\hat{\tau}_{12}^2 + 1) + \hat{\tau}_{12}(\hat{u}_{1i}^2 + \hat{u}_{2i}^2)}{(1-\hat{\tau}_{12})(1+\hat{\tau}_{12})}
    \right)
\end{multlined}
\end{equation}

 Similarly,
\begin{align*}
    r_{p,13} = 
    \frac{1}{\sqrt{N(1+\hat{\tau}_{13}^2)}}\sum_{i=1}^{N}x_{ip} \left( 
    \frac{(\hat{\tau}_{13}^3 - \hat{\tau}_{13}) - \hat{u}_{1i}\hat{u}_{3i}(\hat{\tau}_{13}^2 + 1) + \hat{\tau}_{13}(\hat{u}_{1i}^2 + \hat{u}_{3i}^2))}{(1-\hat{\tau}_{13})(1+\hat{\tau}_{13})}
    \right)
\end{align*}

$$cov(r_{12,p}, r_{13,p}) = E(r_{12,p}r_{13,p}) - E(r_{12,p})E(r_{13,p}) = E(r_{12,p}r_{13,p})$$

 Then, after some algebra,

\begin{equation}
    \begin{multlined}
    \eta_{12,13} = E\sum_{i=1}^{N}x_{ip}^2  \frac{\hat{u}_{i1}^2 \hat{u}_{2i}\hat{u}_{3i}(\hat{\tau}_{12}^2+1)(\hat{\tau}_{13}^2+1)
    + \hat{\tau}_{12}\hat{\tau}_{13} (\hat{u}_{1i}^2 + \hat{u}_{2i}^2)(\hat{u}_{1i}^2 + \hat{u}_{3i}^2)}{N(1-\hat{\tau}_{12}^2)(1-\hat{\tau}_{13}^2)\sqrt{(1+\hat{\tau}_{12}^2)(1+\hat{\tau}_{13}^2)}} \\
    - \frac{\hat{\tau}_{12}(\hat{\tau}_{13}^2 + 1) (\hat{u}_{1i}\hat{u}_{3i})(\hat{u}_{1i}^2 + \hat{u}_{2i}^2) + \hat{\tau}_{13}(\hat{\tau}_{12}^2+1)(\hat{u}_{1i}\hat{u}_{2i})(\hat{u}_{1i}^2 + \hat{u}_{3i}^2)}{N(1-\hat{\tau}_{12}^2)(1-\hat{\tau}_{13}^2)\sqrt{(1+\hat{\tau}_{12}^2)(1+\hat{\tau}_{13}^2)}}
    \end{multlined} 
    \end{equation}
    % + \frac{(1-\hat{\tau}_{12}^2)(1-\hat{\tau}_{13}^2)\hat{\tau}_{12}\hat{\tau}_{13}}{N(1-\hat{\tau}_{12}^2)(1-\hat{\tau}_{13}^2)\sqrt{(1+\hat{\tau}_{12}^2)(1+\hat{\tau}_{13}^2)}}\\
    % - \frac{(1-\hat{\tau}_{12}^2)\hat{\tau}_{12}(\hat{u}_{1i}\hat{u}_{3i}(\hat{\tau}_{13}^2+1)) + 
    % (1-\hat{\tau}_{13}^2)\hat{\tau}_{13}(\hat{u}_{1i}\hat{u}_{2i}(\hat{\tau}_{12}^2+1))}{N(1-\hat{\tau}_{12}^2)(1-\hat{\tau}_{13}^2)\sqrt{(1+\hat{\tau}_{12}^2)(1+\hat{\tau}_{13}^2)}}\\
    % - \frac{(1-\hat{\tau}_{12}^2)\hat{\tau}_{12}\hat{\tau}_{13}(\hat{u}_{1i}^2 + \hat{u}_{3i}^2) + (1-\hat{\tau}_{13}^2) \hat{\tau}_{13}\hat{\tau}_{12}(\hat{u}_{1i}^2 +\hat{u}_{2i}^2)}{N(1-\hat{\tau}_{12}^2)(1-\hat{\tau}_{13}^2)\sqrt{(1+\hat{\tau}_{12}^2)(1+\hat{\tau}_{13}^2)}}

% \begin{equation}\begin{multlined}
% = \frac{(\tau_{23}+2\tau_{12}\tau_{23})(\tau_{12}^2+1)(\tau_{13}^2+1) + \tau_{12}\tau_{13}(6+2(\tau_{12}^2+\tau_{13}^2+\tau_{23}^2))}{(1-{\tau}_{12}^2)(1-{\tau}_{13}^2)\sqrt{(1+{\tau}_{12}^2)(1+{\tau}_{13}^2)}} \\
% - \frac{\tau_{12}(\tau_{13}^2+1)(3\tau_{13}+\tau_{13}+2\tau_{12}\tau_{23}) + \tau_{13}(\tau_{12}^2+1)(3\tau_{12}+\tau_{12}+2\tau_{13}\tau_{23})}{(1-{\tau}_{12}^2)(1-{\tau}_{13}^2)\sqrt{(1+{\tau}_{12}^2)(1+{\tau}_{13}^2)}}\\
% + \frac{(1-\tau_{12})^2(1-\tau_{13}^2)\tau_{12}\tau_{13}}{(1-{\tau}_{12}^2)(1-{\tau}_{13}^2)\sqrt{(1+{\tau}_{12}^2)(1+{\tau}_{13}^2)}}\\
% - \frac{(1-\tau_{12}^2)\tau_{12}\tau_{13}(\tau_{13}^2+1) + (1-\tau_{13}^2)\tau_{13}\tau_{12}(\tau_{12}^2+1)}{(1-{\tau}_{12}^2)(1-{\tau}_{13}^2)\sqrt{(1+{\tau}_{12}^2)(1+{\tau}_{13}^2)}}\\
% - \frac{2(1-\tau_{12}^2)\tau_{12}\tau_{13} + 2(1-\tau_{13}^2)\tau_{13}\tau_{12}}{(1-{\tau}_{12}^2)(1-{\tau}_{13}^2)\sqrt{(1+{\tau}_{12}^2)(1+{\tau}_{13}^2)}}
% \end{multlined} \end{equation}
\begin{equation} \begin{multlined}
= \frac{(\tau_{23}+2\tau_{12}\tau_{13})(\tau_{12}^2+1)(\tau_{13}^2+1) + \tau_{12}\tau_{13}(6+2(\tau_{12}^2+\tau_{13}^2+\tau_{23}^2))}{(1-{\tau}_{12}^2)(1-{\tau}_{13}^2)\sqrt{(1+{\tau}_{12}^2)(1+{\tau}_{13}^2)}} \\
- \frac{\tau_{12}(\tau_{13}^2+1)(4\tau_{13}+2\tau_{12}\tau_{23}) + \tau_{13}(\tau_{12}^2+1)(4\tau_{12}+2\tau_{13}\tau_{23})}{(1-{\tau}_{12}^2)(1-{\tau}_{13}^2)\sqrt{(1+{\tau}_{12}^2)(1+{\tau}_{13}^2)}}\\
% + \frac{3\tau_{12}^3\tau_{13}^3 + \tau_{12}^3\tau_{13} + \tau_{12}\tau_{13}^3 - 5\tau_{12}\tau_{13}}{(1-{\tau}_{12}^2)(1-{\tau}_{13}^2)\sqrt{(1+{\tau}_{12}^2)(1+{\tau}_{13}^2)}}
\end{multlined} \end{equation}
from
$$E(\hat{u}_{i1}^4) = 3, \hspace{5mm} E(\hat{u}_{i1}^3 \hat{u}_{i2}) = 3\tau_{12}, \hspace{5mm} E(\hat{u}_{i1}^2\hat{u}_{i2}^2) = 1 + 2\tau_{12}^2,$$
$$E(\hat{u}_{i1}^2 \hat{u}_{i2} \hat{u}_{i3}) = \tau_{23} + 2\tau_{12}\tau_{13}.$$



% $$\frac{numerator}{(1-\rho_{12}^4)(1-\rho_{13}^2)}$$

% \begin{align*}
%     numerator = &\rho_{12}^4\rho_{13}^2 + \rho_{12}^4 - 2\rho_{12}^3 \rho_{13}^3 - 4 \rho_{12}^3\rho_{13}\rho_{23} + 2 \rho_{12}^3\rho_{13} + \rho_{12}^2\rho_{13}^4 \\
%     & + \rho_{12}^2\rho_{13}^2\rho_{23}^2 + 5\rho_{12}^2\rho_{13}^2 + \rho_{12}^2\rho_{23}^2 - 4\rho_{12}^2 - 4\rho_{12}\rho_{13}^3\rho_{23} + 2\rho_{12}\rho_{13}^3 - 2\rho_{12}\rho_{13} + \rho_{13}^4\\
%     & + \rho_{13}^2\rho_{23}^2 - 4\rho_{13}^2 + \rho_{23}^2 + 3
% \end{align*}

\subsection{Small Sample Correction}
Although the introduced test statistic has convenient asymptotic properties, the sample size is not large enough in many applications. The statistic has its error in the order of $N^{-1}$ \cite{harris1985asymptotic}, and many Monte Carlo experiments show that the test rejects the null hypothesis less frequently than indicated by its nominal size \cite{godfrey1978testing, griffiths1986monte, honda1988size} . In response, Harris (1985) used Edgeworth expansion to obtain the distribution and moment generating function to order $n^{-1}$ of the test statistic \cite{harris1985asymptotic}. Building on this expansion, Honda (1986) and Cribari-Neto and Ferrari proposed corrections to the critical value or to the test statistic that allows better inference even when the sample size is small while preserving the asymptotic properties. \cite{cribari1995improved, cribari2001monotonic, honda1988size}\\


Honda (1988) provided a closed-form formula to adjust the critical value in the order of $O(n^{-1})$ to correct the type I error of the test. This adjustment, only depending on the covariate, sample size, and the degrees of freedom, but not on the data, is a cubic function with respect to $C_{\gamma}$, the critical value at the level of type I error $\gamma$, i.e. $P(\chi_{P}^2 \geq C_{\gamma}) = \gamma$, and we refer to this cubic function as $g$ defined as follows.

\begin{equation}
    \begin{multlined}
g(C_{\gamma}) = C_{\gamma} + C_{\gamma}\bigg(\frac{A_3 - A_2 + A_1}{12NP}\bigg) + C_{\gamma}^2\bigg(\frac{A_2 - 2A_3}{(12NP(P+2)}\bigg) +\\
C_{\gamma}^3 \bigg(\frac{A_3}{12NP(P+2)(P+4)}\bigg) = C_{\gamma} + \tilde{g}(C_{\gamma})
\end{multlined}
\label{hondacorrection}
\end{equation}

 where the scalars $A_1$, $A_2$, and $A_3$ follow the notation of Honda (1988) directly. 

\vspace{5mm}  
One of the desirable properties of $g$ would be monotonicity, because regardless of sample size, we would like to maintain the same ordering of the strength of evidence against the null. This turns out to be almost always true in practice. The derivative of $g(C)$ is 
$$g'(C_{\gamma}) = \frac{A_3}{12NP}\left( \frac{A_3-A_2+A_1+12NP}{A_3} + 
 \frac{2(A_2-2A_3)}{(P+2)A_3}C_{\gamma} + \frac{3}{(P+2)(P+4)}C_{\gamma}^2 \right)$$
 $A_3$ is strictly positive by definition, and we can solve the above quadratic equation to see in which case the derivative is positive \cite{cribari1995improved}. In other words, we can study when the following discriminant is complex.
$$\sqrt{\left(\frac{2(A_2-2A_3)^2}{(P+2)A_3}\right)^2 - 4\cdot\frac{3A_3(A_3-A_2+A_1)}{(P+2)(P+4)A_3} - 4\cdot
\frac{3 \cdot 12NP}{(P+2)(P+4)}}$$
The first two terms inside the square root are $O(1)$ and the last term is $O(n)$, so we can see that the discriminant becomes complex quickly as $n$ increases. Also when the covariates are simulated from normal distribution, $g'(C)$ was always positive unless $n<3$. \\


Similar argument is offered in Cribari (1995) \cite{cribari1995improved}. Based on the correction of the critical value in \ref{hondacorrection}, Cribari (1995) proposes to subtract the correction $\tilde{g}(C_{\gamma})$ so that
$$P(q  \geq g(C_{\gamma})) = P(q \geq C_{\gamma} + \tilde{g}(C_{\gamma})) = P(q - \tilde{g}(C_{\gamma}) \geq C_{\gamma}).$$
This treats the correction as de-biasing instead of changing the overall shape of the distribution. Although this adjustment of the test statistic corrects the size of the test at a given threshold, it prevents further analysis when we combine the test statistics in section \ref{eq:d}.

\vspace{5mm} 
Instead, we aim to adjust the test statistic so that the overall shape of null distribution is closer to $\chi_{P}^2$. We assume a large enough sample size for monotonicity of $g$ and define the inverse function of $g$ to propose the new adjusted test statistic $\tilde{q}_{12}$ = $g^{-1}(q)$
$$\gamma = P(\chi_{P}^2 \geq C_{\gamma}) = P(q \geq g(C_{\gamma})) = P(g^{-1}(q) \geq C_{\gamma})$$
Our final test statistic $\tilde{q}_{12}$ is the real solution to the following equation 
$$q - g(C_{\gamma}) = 0$$
which is guaranteed to be unique by the monotonicity of $g$. The cubic equation can be solved both analytically and numerically efficiently given the covariate $X$. 


\bibliographystyle{plain}
\bibliography{ref}



\end{document}


